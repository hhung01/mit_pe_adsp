18:52:14 From Louis Reid To Everyone:
	Good morning everyone!!!
18:52:31 From Adithya Parvatam To Everyone:
	Good Morning everyone !!
18:52:45 From Eric Bonney To Everyone:
	Good morning everyone
18:52:46 From Angelos Calogirou To All Panelists:
	Good morning!
18:52:54 From Hajime Taniguchi To All Panelists:
	Good morining
18:52:56 From David Zambrano To All Panelists:
	Good morning teacher
18:53:13 From Juan Ciszek To All Panelists:
	Hello everyone!
18:53:24 From Shannon Perez To Everyone:
	Good Morning‚òÄÔ∏è
18:53:38 From Awad Alomari,PE To Everyone:
	Good Afternoon from Doha, Qatar
18:54:19 From Jose Rojas To Everyone:
	Good mormimg!
18:54:27 From Samit Thakur To Everyone:
	Good morning, good afternoon and good evening!
18:54:38 From Hajime Taniguchi To Everyone:
	Good morning!!!
18:54:38 From Kristina miteva To Everyone:
	Good morning!
18:54:48 From Roman Neuhauser To Everyone:
	good afternoon, from Zurich, Switzerland
18:54:57 From Arman Azhand To Everyone:
	hi
18:55:11 From Ketan Kamdar To Everyone:
	Hello
18:55:24 From Juan Camilo P√©rez To All Panelists:
	Good morning, everyone from Bogot√°, Colombia
18:55:34 From Iwan M√ºller To Everyone:
	good afternoon, from Switzerland
18:55:47 From Pedro Ju√°rez Lagos To Everyone:
	Good morning everyone! MX
18:56:09 From Louis Reid To Everyone:
	Hello Switzerland from Boston!!!
18:56:19 From Zainab Saccal To Everyone:
	Afternoon everyone
18:56:22 From Kyle Ragaller To Everyone:
	Hello, from Illinois USA
18:56:35 From Jason Cuevas To Everyone:
	Good morning
18:56:36 From Priyanka Nikam To Everyone:
	Hi Everyone
18:56:46 From Rachana Kotian To Everyone:
	Good morning
18:56:48 From Shel Randall To Everyone:
	Good morning from sunny California! üåû Where the waves are groovy, the avocados are always fresh, and the traffic... well, let's not talk about that! Have a "totally tubular" day, dude! üå¥üèÑ‚Äç‚ôÇÔ∏èüòé
18:57:07 From alberto chico To Everyone:
	Good Morning
18:57:17 From Santiago Arroyo To Everyone:
	Good morning!
18:57:19 From drashti darji To Everyone:
	Good Morning Everyone!
18:57:28 From gilberto scotacci To Everyone:
	good morning
18:57:32 From Frederik Vanaverbeke To Everyone:
	Top of the morning
18:57:35 From abdramane bathily To Everyone:
	Hi Everyone
18:57:37 From Julie Mann To Everyone:
	Good Morning from Nashville!  Where the music never stops playing!
18:57:38 From Leandro Mbarak To Everyone:
	Good Morning t
18:57:49 From Ratna Dhavala To Everyone:
	Good Morning from Boston
18:57:57 From Ali Dawar Abbas To Everyone:
	Good morning
18:58:00 From Mehdi Pirooznia To Everyone:
	Good morning from D.C.
18:58:00 From Rahul Kumar To Everyone:
	Good Morning
18:58:00 From Anil Goud To Everyone:
	Good Morning
18:58:06 From Damian Novo To Everyone:
	Good morning
18:58:10 From Mira Singh To Everyone:
	Good morning everyone
18:58:37 From Fausto Correa To All Panelists:
	Good morning from Mississauga, Canada.
18:58:38 From Matthew Tramel To Everyone:
	Good morning everyone!
18:58:58 From David Palsgrove To Everyone:
	good morning
18:59:06 From Clifford Howlett To Everyone:
	Good morning
18:59:25 From Mario Lemos To All Panelists:
	Hi everyone
18:59:27 From Javier Eduardo M√°rquez Orjuela To All Panelists:
	Good morning everyone
18:59:27 From ruth's iphone To Everyone:
	Good morning all
18:59:33 From Tripureswar Chattopadhaya To Everyone:
	Good Afternoon
18:59:33 From Dru Subramanian To Everyone:
	Hello folks and hoping you are having the best time of the day, wherever you are.
18:59:33 From Deepak Monga To Everyone:
	GM
18:59:37 From Samuel Wright To Everyone:
	GM
18:59:38 From Fernando Matias Gonzalez To Everyone:
	Good morning
18:59:46 From Michael Miller To Everyone:
	Good Morning Everyone
18:59:49 From Jens M√ºller To All Panelists:
	Hi everyone :-)
18:59:49 From Jeremy Boccabello To Everyone:
	Raining in South Carolina üåßÔ∏è
18:59:52 From JUAN RIOS To All Panelists:
	Good morning everyone
18:59:56 From Adriano Oliveira To Everyone:
	Good morning all :-)
18:59:57 From Dru Subramanian To Everyone:
	Good morning to you too, Prof.
18:59:57 From Khadar Sadayan To All Panelists:
	Good Morning
18:59:58 From Mahmoud Mansi To Everyone:
	Hello everyone
19:00:05 From Drea Shaver To Everyone:
	Good Morning
19:00:06 From Mauricio Martinez To All Panelists:
	Good morning
19:00:17 From Barbara Timm-Brock To Everyone:
	Good morning from San Jose CA!
19:00:17 From Paula Valverde To Everyone:
	Good afternoon from Spain
19:00:18 From Thierry Azalbert To Everyone:
	Good evening All from Kuala Lumpur!
19:00:24 From Carlos Eduardo Jauregui Briceno To Everyone:
	Good Morning!
19:00:36 From Stephany Gochuico To Everyone:
	Good afternoon from Paris ;-)
19:00:37 From Julio Rojas To Everyone:
	Good Morning from US Pacific Coast
19:00:45 From Jens M√ºller To Everyone:
	Good afternoon from Hamburg :-)
19:00:46 From Marisol Santillan To Everyone:
	Good afternoon from Oslo :)
19:00:57 From Shel Randall To Everyone:
	Got it. Send questions only to Ankit
19:01:05 From Megha Sharma To Everyone:
	Good Morning from Mexico
19:01:10 From Poonam Gupta To All Panelists:
	Good Morning
19:01:23 From Azaria Berhane To Everyone:
	Good morning from Washington state.
19:01:29 From Julie Mann To Everyone:
	Back to yesterday's lecture:  can you please clarify the difference between closeness and betweenness
19:02:13 From George Selembo To Everyone:
	Good morning! (Charlotte, NC)
19:02:30 From Sophie Voisin To Everyone:
	Good morning
19:02:44 From Michael Miller To Everyone:
	Good Morning (Charlotte NC too)
19:02:48 From Judith Marrugo To Everyone:
	Good morning from Bogot√° Colombia!
19:02:55 From Moderator - Ankit Agrawal To Everyone:
	Q: Back to yesterday's lecture:  can you please clarify the difference between closeness and betweenness	A: Closeness is a measure of how close/ far datapoints are. Betweenness is a type of closeness where removing an edge will break the network
19:02:57 From yuna johnson To All Panelists:
	morning
19:04:52 From LUCIA MUNOZ To Everyone:
	Morning!
19:05:01 From Cristobal Fresno To All Panelists:
	Morning!
19:05:19 From Arturo Gudi√±o Chong To Everyone:
	GM
19:06:42 From Mayda Alkhaldi To All Panelists:
	Good morning from Toronto, Canada üôÇ
19:07:06 From David Palsgrove To Everyone:
	Is there a resource of example questions that are good to ask in different contexts?
19:07:42 From Shel Randall To Everyone:
	Ugh ‚Ä¶ "cut/paste from chat" is disabled. Grr
19:09:12 From Mayda Alkhaldi To Everyone:
	Good morning from Toronto, Canada üôÇ
19:10:03 From Daniel Gibson To Everyone:
	Distances between points are meaningless
19:10:06 From Cristobal Fresno To All Panelists:
	because it does dimensionality reduction
19:10:08 From Lalit Vyas To Everyone:
	No meaning of distance.
19:10:12 From Leandro Mbarak To Everyone:
	because is related with distributions
19:10:15 From Jeremy Boccabello To Everyone:
	I was wondering why it didn‚Äôt show up in the case studies
19:10:22 From Varuni Rao To Everyone:
	because it uses only first 2 PCs and not the entire data?
19:10:22 From Roman Neuhauser To Everyone:
	there are not original features of the dataset
19:10:24 From Sellathurai Sriganesha To Everyone:
	SNE can produce different clusters when run multiple times
19:10:32 From Adithya Parvatam To Everyone:
	Its a dimensionality reduction for visulation
19:10:35 From Uzair Akram To Everyone:
	Same as with the PCA - difficult to interpret
19:10:36 From Julie Mann To Everyone:
	many outcomes?
19:10:37 From Hanna Purnomo To Everyone:
	because it's a reduction technique, not a clustering
19:10:40 From Alisher Jardemaliyev To Everyone:
	Its a technique specifically designed for visualizing high-dimensional datasets
19:10:40 From Jason Cuevas To Everyone:
	Categories given = supervised
19:10:43 From Sharon Kuang To Everyone:
	SNE is based on tagged data
19:10:49 From Paula Valverde To Everyone:
	Do not generate cluster assignment
19:11:02 From Daniel Gibson To Everyone:
	Inconsistent outcomes.  Lack of actual identification of the groups aside from by-eye discernment
19:11:06 From Adithya Parvatam To Everyone:
	We cannot for sure tell these ar real clusters
19:11:18 From Daniel Gibson To Everyone:
	Cannot post-hoc sort a given item into existing groups
19:11:32 From Daniel Bautista To Everyone:
	you'll need to run it again?
19:11:33 From Daniel Gibson To Everyone:
	Re-run
19:11:39 From Shel Randall To Everyone:
	have to start over
19:11:42 From Kyle Ragaller To Everyone:
	clustering is grouping by known data, t-sne is potentials of correlation
19:11:43 From Shilpa Murthy To Everyone:
	You need to rerun with different results
19:11:49 From Megha Sharma To Everyone:
	We will re-run it again
19:11:49 From Jeremy Boccabello To Everyone:
	run the whole set
19:12:53 From Roman Neuhauser To Everyone:
	Can we also use PCA to do a clustering, in case we are working with high dimensional data?
19:13:06 From Deepak Monga To All Panelists:
	Can you please make the concepts clear for SNE and PCA?  I am not clear
19:13:32 From Leandro Mbarak To Everyone:
	can we use the number or groups in t-sne as an input for cluster analysis?
19:13:37 From Seher Rehan To Everyone:
	So how 23 and me guys do they run it every single time and data changes?
19:13:43 From Judith Marrugo To Everyone:
	But clients won't like that because it is more money
19:13:49 From Shilpa Murthy To Everyone:
	Can you use PCA and tSNE for dimension reduction and then do cluster analysis
19:13:51 From Judith Marrugo To Everyone:
	How do you manage that ?
19:13:56 From Sophie Voisin To Everyone:
	Is recomputing would happen too for unsurpervised clustering?
19:13:57 From Dom Lazara To Everyone:
	Can't a new point start a new cluster?
19:13:58 From Barbara Timm-Brock To Everyone:
	Does that mean that SNE is not a predictive method?
19:14:14 From Raj Desikavinayagompillai To Everyone:
	Clustering - segmentation  are they same or different
19:14:20 From Shel Randall To Everyone:
	I keep wanting to just find local centers-of-mass. Why doesn't that work?
19:14:51 From Yasir Maqbool To Everyone:
	What about the elbow method to find the number of clusters instead of tsne?
19:15:03 From Paula Valverde To Everyone:
	Is there any tradeoff rule for removing/non removing OUTLIERs before clustering?
19:15:11 From Santiago Arroyo To Everyone:
	Does this means that PCA is also unsupervised learning?
19:15:20 From Luca Mastrantoni To Everyone:
	Which other methods are usually used for pre-processing? apart from PCA
19:15:34 From Uzair Akram To Everyone:
	@Yasir there are other methods to select the number of clusters too
19:16:07 From Daniel Gibson To Everyone:
	@Paula, they may represent a legitimate, but smaller group.  Depends on the context if you want to remove them.
19:16:36 From Angelos Calogirou To All Panelists:
	What about SOMs or Kohonen maps? Do they help pre-process data??
19:16:42 From Angalar Chi To Everyone:
	it depends on your need
19:16:43 From Daniel Gibson To Everyone:
	% of variance captured
19:16:45 From Roman Neuhauser To Everyone:
	the ones that show the highest degree of variability
19:16:48 From Jeremy Boccabello To Everyone:
	percentage of variance
19:16:49 From Anil Goud To Everyone:
	The most varied
19:16:50 From LUCIA MUNOZ To Everyone:
	correlation
19:16:51 From Santiago Arroyo To Everyone:
	Caaching 70% variance
19:16:51 From Angelo Rossetti To Everyone:
	components with highest variability
19:16:53 From Herman Gothe To Everyone:
	maximum variance
19:16:56 From Prashant Bapuji To Everyone:
	variability is explained
19:16:58 From Daniel Gibson To Everyone:
	number of groups expected (disease states)
19:16:58 From Nohelia Osorio To Everyone:
	depending of the distance
19:17:00 From Louis Reid To Everyone:
	based on variance
19:17:04 From Lalit Vyas To Everyone:
	Elbow method‚Ä¶ as soon as the significance goes down, we stop
19:17:05 From Uzair Akram To Everyone:
	Define the threshold of variance explanation
19:17:08 From Luca Mastrantoni To Everyone:
	analyzing the scree plot
19:17:09 From Ian Gault To Everyone:
	Elbow method?
19:17:11 From Mira Singh To All Panelists:
	Max variation explained
19:17:12 From Yasir Maqbool To Everyone:
	feature scoring
19:17:12 From Michael Wahnich To Everyone:
	marginal explanatory power of additional variables
19:17:16 From Thierry Azalbert To Everyone:
	Calaculate what % of the variance is explained by the first N principal components
19:17:21 From Priyanka Nikam To Everyone:
	Are you changing the slides?
19:17:21 From ashish kumar To Everyone:
	when the variance change doesn't change by dropping PC
19:17:21 From Megha Sharma To Everyone:
	Variablity high variability than move doen
19:17:34 From Shilpa Murthy To Everyone:
	How do we measure this?
19:17:42 From Daniel Gibson To Everyone:
	Future measurability of the PCs
19:18:41 From Alisher Jardemaliyev To Everyone:
	Morpheus says correlation matrix is good
19:19:02 From Moderator - Ankit Agrawal To Everyone:
	Q: Can we also use PCA to do a clustering, in case we are working with high dimensional data?	A: No. PCA isn't looking for distance between points and grouping them together, it is trying to find the direction in which variance is maximized.
19:19:16 From Daniel Bautista To Everyone:
	Ooh cool
19:19:24 From Daniel Gibson To Everyone:
	Eigenvalue > mean +/- stddev
19:19:30 From Daniel Gibson To Everyone:
	2x stdev
19:19:46 From Roman Neuhauser To Everyone:
	1
19:19:47 From Louis Reid To Everyone:
	Standard Error
19:19:48 From Varuni Rao To Everyone:
	1
19:19:50 From Barbara Timm-Brock To Everyone:
	1
19:19:53 From Fausto Correa To Everyone:
	1
19:19:59 From Adithya Parvatam To Everyone:
	1
19:20:19 From ankita srivastava To All Panelists:
	1
19:20:21 From Eric White To Everyone:
	more than 1
19:20:25 From Shilpa Murthy To Everyone:
	> 1
19:20:43 From Moderator - Ankit Agrawal To Everyone:
	Q: What about the elbow method to find the number of clusters instead of tsne?	A: Yes, t-SNE, elbow method, silehoutte score are different methods to identify number of clusters. Some clustering methods are capable of finding the number of clusters automatically like DBSCAN
19:21:06 From ashish kumar To Everyone:
	would that equate to 1/2 of PCs?
19:21:17 From Mayda Alkhaldi To Everyone:
	So what is the difference between PCA and K-means clustering?
19:21:18 From Moderator - Ankit Agrawal To Everyone:
	Q: Does that mean that SNE is not a predictive method?	A: Yes, t-SNE is a visualization method, not a predictive method
19:21:23 From Luca Mastrantoni To Everyone:
	is this the kaiser criterion?
19:21:41 From Shel Randall To Everyone:
	Ashish - no because mean does not equal median
19:21:57 From ashish kumar To Everyone:
	TY
19:22:34 From Shel Randall To Everyone:
	elbow method : if they argue with you about your math, you hit them with your elbow. :)
19:22:40 From Raj Desikavinayagompillai To Everyone:
	Clusters vs segmentation are they same or different
19:22:47 From Moderator - Ankit Agrawal To Everyone:
	Q: I keep wanting to just find local centers-of-mass. Why doesn't that work?	A: There could be situation where points maybe close to each other in a local setting but in a global setting, they may belong to different cluster (like the image on the slides) so local methods don't work very well
19:22:59 From Uzair Akram To Everyone:
	@Shel haha pretty sure client will fire us if we do that
19:23:07 From Daniel Gibson To Everyone:
	Concentric items like this were considered tricky for most of the methods discussed.  If the basis was changed to radial coordinates, would that open new methods?
19:23:28 From Leandro Mbarak To Everyone:
	Q: i don't undestand have to use PCA as a preprocessing using test of average of eigenvalues
19:24:11 From Moderator - Ankit Agrawal To Everyone:
	Q: Is there any tradeoff rule for removing/non removing OUTLIERs before clustering?	A: Removing outliers can help do clustering better but most methods will assign the outliers to their own clusters i.e. the number of clusters will increase.
19:24:19 From simona tenaglia To Everyone:
	dimensione are given by variables in a datasets?
19:24:31 From Daniel Gibson To Everyone:
	@Simona, yes
19:24:42 From Moderator - Ankit Agrawal To Everyone:
	Q: Does this means that PCA is also unsupervised learning?	A: Dimensionality reduction and clustering are both types of unsupervised learning.
19:26:16 From Moderator - Ankit Agrawal To Everyone:
	Q: Which other methods are usually used for pre-processing? apart from PCA	A: Depends on the objective. Missing value handling, outlier detection, etc are also form of preprocessing. Clustering methods are also a type of analysis and preprocessing
19:26:29 From Thierry Azalbert To Everyone:
	we compare distances between points
19:26:32 From Sophie Voisin To Everyone:
	Compute a distance
19:26:34 From Shilpa Murthy To Everyone:
	Identify points that are nearby
19:26:35 From Fernando Schmidkonz To Everyone:
	The ones that are closer to each other
19:26:36 From Neha Purohit To All Panelists:
	Id distance between them is less than threshold
19:26:36 From Sujasha Gupta To Everyone:
	Calculate the centroids
19:26:39 From Wilson Castiblanco Quintero To All Panelists:
	density
19:26:41 From Mackenzie Hunt To Everyone:
	Coordinates
19:26:43 From ashish kumar To Everyone:
	distance between points
19:26:45 From Varuni Rao To Everyone:
	shortest distances same cluster
19:26:48 From Megha Sharma To Everyone:
	Distances
19:26:49 From David Enck To Everyone:
	distance between and within clusters
19:26:51 From Roman Neuhauser To Everyone:
	we compare them to each dimension
19:26:51 From Carlos Bertagnolli To Everyone:
	centrality methods
19:26:54 From Julie Mann To Everyone:
	distance and scale
19:26:59 From Raj Desikavinayagompillai To Everyone:
	Adjacent matrixes
19:27:01 From Ricardo Vides To Everyone:
	first we need to define the number of clusters "K"
19:27:03 From Shannon Perez To All Panelists:
	I-means
19:27:04 From Paula Valverde To Everyone:
	centers
19:27:11 From Moderator - Ankit Agrawal To Everyone:
	Q: Calaculate what % of the variance is explained by the first N principal components	A: In python, the "pca.explained_variance_" method can give you the amount of variance captured by each component sorted in decreasing order
19:27:29 From Gayathri Ramaswamy To Everyone:
	Density Based Clustering
19:27:32 From Shannon Perez To Everyone:
	K-means
19:27:34 From Prashant Bapuji To Everyone:
	data is in different units it needs to be normalized/standardize
19:27:39 From Lalit Vyas To Everyone:
	This implies one point per customer‚Ä¶ how do we reach to that level‚Ä¶ ex: aggregate the data points per customer? How do we do that without losing valuable patterns?
19:27:45 From Daniel Gibson To Everyone:
	A network of edge with shortest edges, constructed serially.
19:28:07 From Herman Gothe To Everyone:
	how do you define how much is close?
19:28:29 From Sujasha Gupta To Everyone:
	Kepp updating the distance calculated each time
19:28:32 From Thiago Barros To Everyone:
	vector span?
19:28:36 From Raj Desikavinayagompillai To Everyone:
	How do u define the size of cluster. Example ages - do we put histograms of size 5 or 10 or 20 as a cluster?
19:29:37 From EUNICE HERNANDEZ To Everyone:
	Can you define a distance threshold for closeness?
19:29:47 From Luis Gaxiola To Everyone:
	What is the point of having a final meta-cluster that encompasses all other subclusters?
19:29:54 From Neha Purohit To All Panelists:
	How do we program when to stop
19:29:59 From Shel Randall To Everyone:
	so the trick is knowing where to stop
19:30:00 From Daniel Gibson To Everyone:
	@Eunice, maybe average distance as it is growing compared to the next connection.  When that next connection is very big compared to the previous connection.
19:30:03 From Louis Reid To Everyone:
	How do you deal with time series data and clustering?
19:30:17 From Luis Gaxiola To Everyone:
	sub clusters I understand, but a whole final holistic cluster in hierarchical clustering, what is the purpose?
19:30:26 From Fernando Matias Gonzalez To Everyone:
	also clustering by similarity
19:30:26 From Merrill Kashiwabara To All Panelists:
	Stop when # of clusters starts to decrease.
19:30:31 From Sujasha Gupta To Everyone:
	How to define a threshold for distance for closeby points?
19:30:49 From Herman Gothe To Everyone:
	what about the units of measure?
19:30:51 From Dom Lazara To Everyone:
	What does it mean "to keep increasing the distances"?
19:30:54 From Judith Marrugo To Everyone:
	If u separate the closters what is the goal ?
19:30:55 From Moderator - Ankit Agrawal To Everyone:
	Q: how do you define how much is close?	A: Each point has to belong to some cluster so the point that has smallest distance to a point which is already assigned to a cluster is considered as close. There is no specific threshold value
19:30:57 From Shannon Perez To Everyone:
	DBSCAN
19:31:24 From Swati Chandna To Everyone:
	Do you apply one or other clustering method on a given dataset?
19:31:26 From Daniel Gibson To Everyone:
	@Dom with the hierarchal method, each new connection is longer than the previous.
19:31:56 From Hugo Escandon To Everyone:
	set a centroid
19:32:00 From Hugo Escandon To Everyone:
	3
19:32:31 From Carlos Bertagnolli To Everyone:
	Start at multiple random points to avoid some sort of bias?
19:32:43 From Prashant Bapuji To Everyone:
	is center a datapoint?
19:32:52 From Varuni Rao To Everyone:
	what if we end up selecting two centers close to each other?
19:32:54 From Shel Randall To Everyone:
	I feel like Rayleigh criterion is going to appear someplace
19:32:54 From Sophia Arellano To Everyone:
	how to find the centers?
19:32:56 From Trang Nguyen To Everyone:
	what are other methods to pre-process the data prior to clustering besides PCA?
19:32:59 From Sujasha Gupta To Everyone:
	'How to define the center point
19:33:00 From Hugo Escandon To Everyone:
	it could be for some algorithms
19:33:09 From Swati Chandna To Everyone:
	How do you put a center programmatically? Do you first randomly divide into three groups and then find centroid for each of them
19:33:31 From Herman Gothe To Everyone:
	what about units of measure? distance will change dramatically?
19:33:51 From ashish kumar To Everyone:
	how to determine where to position the centers in the clusters
19:34:00 From Lalit Vyas To Everyone:
	Maximizing or minimizing between points?
19:34:06 From Shannon Perez To All Panelists:
	How many iterations of finding the centers do you need to try before you can be sure you have the best fit.
19:34:12 From Sellathurai Sriganesha To Everyone:
	.
19:34:13 From Hugo Escandon To Everyone:
	also distances should standarized/normalized
19:34:19 From Alisher Jardemaliyev To Everyone:
	how do we know that its a center?
19:34:42 From Lalit Vyas To Everyone:
	Clear, thanks!
19:34:45 From Moderator - Ankit Agrawal To Everyone:
	Q: Do you apply one or other clustering method on a given dataset?	A: Yes, each clustering method has a set of advantages and disadvantages. Based on the data and objective, we choose a subset of clustering methods
19:34:52 From Shel Randall To Everyone:
	I'm wondering how much of this is theory, vs. how much we have to actually use ourselves, vs. how much the software does for us
19:35:39 From Daniel Gibson To Everyone:
	@Shel software will do the work, but each approach has limitations.  You need to know those limits when choosing or when troubleshooting.
19:35:40 From Wilson Castiblanco Quintero To All Panelists:
	Why the centroids are necessary if by eye-sight it is clear we have 3 clusters? are those created for performance when measure points?
19:35:47 From Thiago Barros To Everyone:
	how to verify if a clustering was performed correctly?
19:35:49 From Azaria Berhane To Everyone:
	Can you  give a real world example of how you would use these strategies?
19:35:49 From Shannon Perez To Everyone:
	How many iterations of finding the centers do you need to try before you can be sure you have the best fit?
19:36:05 From Moderator - Ankit Agrawal To Everyone:
	Q: is center a datapoint?	A: Not always. PAM method enforces a center to be a datapoint. Hierarchical clustering doesn't rely on finding a center.
19:36:31 From Yasir Maqbool To Everyone:
	Apart from the ML clustering model, how do you see a marketing clustering model like RFM? When should we adopt ML and when RFM is better?
19:36:32 From ashish kumar To Everyone:
	do we run multiple clustering algorithms to confirm the clusters?
19:36:45 From Shel Randall To Everyone:
	@Daniel Gibson : that makes sense. Just wanted to understand how much I have to internalize this
19:37:06 From Daniel Gibson To Everyone:
	@Shel remember until at least next Wednesday‚Ä¶ ;)
19:37:17 From Lalit Vyas To All Panelists:
	+1 on Yasir‚Äôs question about RFM vs. ML
19:38:14 From Wilson Castiblanco Quintero To All Panelists:
	What happen if SNE doesn‚Äôt show clear clusters?
19:38:34 From Wilson Castiblanco Quintero To All Panelists:
	PCE should be used instead?
19:38:37 From Thiago Barros To Everyone:
	how do we verify the number of clusters defined is correct? (in a study, for example)
19:39:42 From Daniel Gibson To Everyone:
	@Thiago if it is a study, the clusters might form the next hypothesis that you‚Äôd have to test.
19:42:06 From Sujasha Gupta To Everyone:
	How far should these centers be from each other?
19:42:31 From Varuni Rao To Everyone:
	won't the clusters depend on the choice of centroid location?
19:42:39 From Lalit Vyas To Everyone:
	@sujasha - doesn‚Äôt matter ‚Ä¶ randomly placed
19:43:06 From Petrino Ippolito To Everyone:
	Are there ever situations where this continues to loop forever?
19:43:06 From Shel Randall To Everyone:
	@Varuni - yes, but they change with each iteration until they find equilibrium
19:43:08 From Jens M√ºller To Everyone:
	But how do I decide where to put the centers at‚Ä¶.?
19:43:34 From Raj Desikavinayagompillai To Everyone:
	How do u represent a data point. Is it two dimensional (x,y) or multi dimensional?
19:43:42 From Varuni Rao To Everyone:
	@Shel thanks but then how do you find equilibrium?
19:43:42 From Shilpa Murthy To Everyone:
	@varuni centroid is calculated at the end of each iteration. The clusters would depend on the first choice of the points we choose as the center
19:43:44 From Moderator - Ankit Agrawal To Everyone:
	Q: what about units of measure? distance will change dramatically?	A: Most of the times, the data is first normalized so the scale and units are all equal, before we apply PCA or clustering methods.
19:43:51 From Soumyo Banerjee To Everyone:
	Does the value of K change - perhaps suggested by the algorithm?
19:43:52 From Shannon Perez To Everyone:
	So the number of iterations is determined by the algorithm
19:44:03 From Santiago Arroyo To Everyone:
	How do you know what is the optimal K?
19:44:27 From Louis Reid To Everyone:
	No hypothesis testing to understand significance
19:44:29 From Soumyo Banerjee To Everyone:
	If we chose wrong K - it will provide incorrect clustering
19:44:30 From Shel Randall To Everyone:
	@Varuni - the sequence finds its own equilibrium closer and closer with each iteration
19:44:31 From Michael Wahnich To Everyone:
	why would clusters have similar radii ?
19:44:36 From Claudio Chaves To Everyone:
	Can we set the min number of points in a cluster?
19:44:43 From Hugo Escandon To Everyone:
	elbow
19:44:58 From Varuni Rao To Everyone:
	thanks Shel
19:45:03 From Daniel Gibson To Everyone:
	Is there a theoretical minimum of clusters per unit dimension?  It seems that you definitely want more than 1, but would you seek only 2 groups with 100‚Äôs of dimensions?
19:45:10 From Moderator - Ankit Agrawal To Everyone:
	Q: How many iterations of finding the centers do you need to try before you can be sure you have the best fit.	A: Until the center or the points in clusters converges i.e. doesn't change between iterations. In python, we can also set a termination condition under "max_iter" parameter
19:45:18 From Raj Desikavinayagompillai To Everyone:
	How do u represent a datapoint in k-cluster - is it two dimensional (x,y) or multi dimensional
19:45:28 From ashish kumar To Everyone:
	in this slide, visually the 3 clusters would be different from what the final clusters are, right?
19:45:50 From Lalit Vyas To Everyone:
	I‚Äôm interested to know how to get to n-dimensional data from transactional data without losing patterns‚Ä¶
19:45:55 From Shilpa Murthy To Everyone:
	So similar radii is an effect of choosing euclidean distance to calculate the distance to minimize on?
19:45:58 From Moderator - Ankit Agrawal To Everyone:
	Q: Why the centroids are necessary if by eye-sight it is clear we have 3 clusters? are those created for performance when measure points?	A: Read world data is not 2D but 100s of dimensions where can't do "eye-sight" analysis
19:47:27 From Joshua Coleman To Everyone:
	@Shilpa similar distances is a result of point-to-center minimizing.  If one sphere is larger to get to point 'n' than a neighboring sphere, the neighbor sphere will take it.  Your result is all spheres being similar in size
19:47:39 From TP Singh To Everyone:
	3
19:47:43 From Roman Neuhauser To Everyone:
	3
19:48:02 From Herman Gothe To Everyone:
	decrease
19:48:04 From Daniel Gibson To Everyone:
	approaches 0
19:48:05 From Anil Goud To Everyone:
	Goes to 0
19:48:05 From Johan Goedkoop To Everyone:
	too expensive
19:48:06 From David Enck To Everyone:
	keep getting smaller
19:48:06 From Sujasha Gupta To Everyone:
	Decreaes
19:48:07 From Carlos Bertagnolli To Everyone:
	approach a constant
19:48:07 From Roman Neuhauser To Everyone:
	decreases
19:48:11 From Arturo √Åvalos To All Panelists:
	There a limit
19:48:13 From Rahul Kumar To Everyone:
	0
19:48:13 From lynn liang To Everyone:
	decrease to 0
19:48:16 From Leandro Mbarak To Everyone:
	if k = n the sum is low but is not usefull
19:48:16 From Dom Lazara To Everyone:
	eventually each point is a cluster
19:48:21 From Mario Lemos To All Panelists:
	Is possible to have outliers  on clusters ? How to deal with them?
19:48:35 From Daniel Gibson To Everyone:
	Each sample is a cluster...
19:48:36 From David Enck To Everyone:
	like over fitting a model, R2 goes up
19:48:40 From Moderator - Ankit Agrawal To Everyone:
	Q: What happen if SNE doesn‚Äôt show clear clusters?PCE should be used instead?	A: PCA is not a clustering method. if t-SNE doesn't give clear clusters, we can use other clustering methods like K-Means, PAM, Hierarchical, DBSCAN, OPTICS, Spectral, GMM, etc
19:48:56 From Julio Rojas To Everyone:
	objective is to minimize dimension of data
19:48:59 From Joshua Coleman To Everyone:
	likely choose an inflection point
19:48:59 From Shilpa Murthy To Everyone:
	@Joshua thanks I was intuitively thinking about the spherical shape more than the similar size but makes sense
19:49:45 From Moderator - Ankit Agrawal To Everyone:
	Q: Are there ever situations where this continues to loop forever?	A: It is and most software libraries (SAS, Python, R, Java, scala, etc) have an upper limit on how many times we run it. Typically we force a termination after 1000 iterations
19:50:03 From Shannon Perez To Everyone:
	It has a steep drop to 3 and then levels off
19:50:06 From Daniel Gibson To Everyone:
	point of inflection
19:50:14 From Wilson Castiblanco Quintero To All Panelists:
	Is there a way to calculate that automatically? Or always has to be visually?
19:50:47 From Anil Goud To Everyone:
	Does the number of dimensions always reflect the number of variables?
19:51:02 From Luis Gaxiola To Everyone:
	Can we consider it from a mathematically standpoint calculating a change of slope?
19:51:07 From Alisher Jardemaliyev To Everyone:
	what do you mean by "Run K-means clustering for several number of groups K"???
19:51:11 From Varuni Rao To Everyone:
	what if new data is added to the model after the k-means clustering is done and they form a new cluster, will the model accommodate it?
19:51:14 From Roman Neuhauser To Everyone:
	Can we say the Clusters are Labels?
19:51:16 From Joshua Coleman To Everyone:
	that was my though Luis
19:51:26 From Daniel Gibson To Everyone:
	@Luis, yes when derivative crosses 0
19:51:27 From Santiago Arroyo To Everyone:
	How do you calculate "distance" given all the dimensions?
19:51:33 From Shel Randall To Everyone:
	@Luis - that seems like that would work
19:51:39 From Daniel Gibson To Everyone:
	Double derivative.
19:51:45 From Luis Gaxiola To Everyone:
	@Daniel, exactly.  That's what I thought, with a derivative
19:51:54 From Sophie Voisin To Everyone:
	You may still want to have an automated way to do all of this even with a lower dimensions
19:52:01 From Shel Randall To Everyone:
	is it possible to not have a distinctive "elbow" for a dataset?
19:52:20 From Alisher Jardemaliyev To Everyone:
	what does it mean "Run K means clustering for several number of groups K"?
19:52:26 From Himansu Jena To Everyone:
	Will double derivative work mathematically to find out high drop for K value?
19:52:33 From Moderator - Ankit Agrawal To Everyone:
	Q: Is there a theoretical minimum of clusters per unit dimension? It seems that you definitely want more than 1, but would you seek only 2 groups with 100‚Äôs of dimensions?	A: Number of clusters is not dependent on the number of dimensions in the data but on the similarity and dis-similarity between different data points.
19:52:35 From Julie Mann To Everyone:
	what if the clusters are of different densities?
19:53:11 From Daniel Gibson To Everyone:
	@Himansu, I think so, since the change in the change approaches 0
19:53:30 From Shilpa Murthy To Everyone:
	Double derivative will still give you a local optima but it would be a great method to find the first few k
19:53:40 From Shilpa Murthy To Everyone:
	Sometimes the change oscillates
19:54:08 From Anil Goud To Everyone:
	In data science do we not identify and remove outliers first?
19:54:14 From Joshua Coleman To Everyone:
	@Julie, no change for k-means, it only cares about similar size spheres
19:54:20 From Moderator - Ankit Agrawal To Everyone:
	Q: How do you calculate "distance" given all the dimensions?	A: You can compute L-norm, Eucledian, Manhattan, Jacobian, Cosine, Cheby-shev, etc distance metrics. These methods work on any number of dimensions.
19:54:22 From Daniel Gibson To Everyone:
	@Shilpa true with real data, so you‚Äôd set a noise threshold for the change.
19:55:15 From Jeremy Boccabello To Everyone:
	is this the same as KMedoid?
19:56:07 From Luis Gaxiola To Everyone:
	@Jardimaliyev, you need to compute the square distances for different proposed values of K in that heuristic, that is what it means, so you can compare them later.
19:56:19 From Moderator - Ankit Agrawal To Everyone:
	Q: is this the same as K-Medoid?	A: Yes, PAM method is also called K-medoid method.
19:56:20 From Fernando Schmidkonz To Everyone:
	can you provide an example of how much computational time is impacted by choosing PAM?
19:56:56 From Daniel Gibson To Everyone:
	Isn‚Äôt PAM bordering on network analysis?
19:57:26 From Petrino Ippolito To Everyone:
	So centroid is to mean what PAM is to median, loosely speaking?
19:57:32 From Moderator - Ankit Agrawal To Everyone:
	Q: what if new data is added to the model after the k-means clustering is done and they form a new cluster, will the model accommodate it?	A: We'll have to "train" k-means again. If we are only predicting the cluster, then the new datapoints are forced to be assigned to 1 of the clusters that the algorithm has learnt. To learn a new cluster, we'll have to retrain the model
19:57:34 From Jakub Baranowski To Everyone:
	Do we then move cluster centers in PAM? If so do we move it to the closest observation in the direction we would move it with k-means?
19:57:40 From Santiago Arroyo To Everyone:
	How do you initial cluster centers to avoid they are close to each other, if iit is random?
19:57:44 From Santiago Arroyo To Everyone:
	choose
19:58:08 From ashish kumar To Everyone:
	is medoid still randomly chosen?
19:58:27 From Shannon Perez To Everyone:
	Do you choose the medoids or are they found by PAM
19:59:01 From Ricardo Vides To Everyone:
	can you show a visual example of how to select a medoid?
20:00:09 From Daniel Gibson To Everyone:
	That little line above the 3rd graph is bothering me.  I thought it was crud on my screen.
20:00:15 From Shel Randall To Everyone:
	or ‚Ä¶ books ABOUT sports!
20:00:19 From Jeremy Boccabello To Everyone:
	I think Medoids are randomly chosen then the algorithm shifts to the next medoid closer to the cluster mean.
20:00:20 From Moderator - Ankit Agrawal To Everyone:
	Q: How do you initial cluster centers to avoid they are close to each other, if it is random?	A: K-means and PAM choose it at random but there are other variations to k-means like k-means++ that focuses on choosing the initial centers more carefully.
20:00:33 From Luis Gaxiola To Everyone:
	@Jakub, great question
20:00:43 From alberto chico To Everyone:
	all this methods complement each other. how can you integrate the output of each method to have a more complete unified picture
20:00:45 From alberto chico To Everyone:
	?
20:02:57 From Megha Sharma To Everyone:
	So for Gaussian mixture model - we can have have single variable in 2 or more cluster?
20:02:59 From Shel Randall To Everyone:
	it is as much art as science I think. being familiar with the kin dof data you're looking at certainly helps
20:03:11 From alberto chico To Everyone:
	what are the benefits of ellipsoidal shaped clusters?
20:03:19 From Shel Randall To Everyone:
	@Megha - yes, each in its own proportion
20:03:21 From Moderator - Ankit Agrawal To Everyone:
	Q: So centroid is to mean what PAM is to median, loosely speaking?	A: PAM is a clustering algorithm, not a metric like mean or centroid. A better way would be "centroid is to mean like median is to medoid"
20:03:41 From Megha Sharma To Everyone:
	Thanks @Shel
20:03:45 From Daniel Gibson To Everyone:
	@alberto, it is not about advantage, it is just if the data happen to cluster that way they might not be seen as a cluster with k-means.
20:04:11 From alberto chico To Everyone:
	thank you @Daniel
20:04:49 From Mira Singh To Everyone:
	@Ankit Agarwal that was helpful
20:04:52 From Moderator - Ankit Agrawal To Everyone:
	Q: So for Gaussian mixture model - we can have have single variable in 2 or more cluster?	A: Single data-point can belong to 2 or more clusters. Clustering is not done based on number of variables but based on number of data points. Each data point can be represented by 100s of variables/ dimensions
20:05:29 From EUNICE HERNANDEZ To Everyone:
	When would you choose a method over the other?
20:05:35 From Gaetano Caolo To All Panelists:
	How to know what methods should be used for clustering?
20:05:38 From Petrino Ippolito To Everyone:
	Would it be appropriate to opt for Gaussian Mixture Model if you're expecting clusters to be non-mutually exclusive, and K-means if we expect mutually exclusive clusters (like identifying handwritten digits)?
20:05:38 From Megha Sharma To Everyone:
	So how will we arrange them in the graph @Ankit ?
20:06:54 From Moderator - Ankit Agrawal To Everyone:
	Q: So how will we arrange them in the graph @Ankit ?	A: I don't know what this means. We are not talking about graphs right now but data points in high dimensional space.
20:07:08 From ashish kumar To Everyone:
	how does Gaussian mixture model handle outliers?
20:07:46 From EUNICE HERNANDEZ To Everyone:
	Is it fair tosay that if the data set is very large you go for PAM?
20:07:56 From Megha Sharma To Everyone:
	Ok thanks @Ankit
20:08:12 From Moderator - Ankit Agrawal To Everyone:
	Q: Would it be appropriate to opt for Gaussian Mixture Model if you're expecting clusters to be non-mutually exclusive, and K-means if we expect mutually exclusive clusters (like identifying handwritten digits)?	A: There are other methods too. Hierarchical, Spectral, PAM also work for mutually exclusive clusters. DBSCAN, OPTICS work for non-mutually exclusive like GMM.
20:08:20 From Jason Cuevas To Everyone:
	Are there cases where running Gaussian and the PAM with optimal clustering number makes sense
20:08:35 From Jason Cuevas To Everyone:
	Then PAM
20:08:40 From Daniel Gibson To Everyone:
	Processing power and time needed seem to be the biggest deciding point.
20:08:45 From alberto chico To Everyone:
	You can't use this methods to complement each other because of the cost?
20:08:47 From Megha Sharma To Everyone:
	How will we able to check the no of cluster in Gaussian mixture model
20:08:52 From Judith Marrugo To Everyone:
	How we measure the datasets related to Gaussian mixture model?
20:08:54 From Ben Germany To Everyone:
	I know large data set is relative but when is a time when you chose not to use Gaussian model due to size of data set
20:09:04 From Alisher Jardemaliyev To Everyone:
	can we use all these models in python??
20:09:41 From Shel Randall To Everyone:
	@Ben : I think if your program has been running for a week, you are probably using the wrong method. :)
20:10:03 From Ben Germany To Everyone:
	Haha thanks
20:10:24 From Daniel Gibson To Everyone:
	When you start to worry.  ‚ÄúIs my computer frozen, or is it still computing?‚Äù
20:10:32 From Shel Randall To Everyone:
	lol
20:10:46 From drashti darji To Everyone:
	@alisher , Yes, I think prof just said , we can use all models in oythin.
20:10:48 From Raj Desikavinayagompillai To Everyone:
	Can I say it is building a graph is also same as clustering?
20:10:51 From Moderator - Ankit Agrawal To Everyone:
	Q: can we use all these models in python??	A: Yes. There are more methods (that we don't talk about today) that are also available in Python. You can find them here: https://scikit-learn.org/stable/modules/clustering.html
20:10:59 From Prashant Bapuji To Everyone:
	as the data is not lable ,how do you determine or categorize it as hierarchical clustering
20:11:01 From Shel Randall To Everyone:
	that's when you want a system that shows blinky lights while it's thinking
20:11:36 From Daniel Gibson To Everyone:
	Direct from CPU, not S/W interpretation!
20:11:37 From Michael Miller To Everyone:
	visually c looks closer to de
20:11:46 From Vikas Srivastava To Everyone:
	Would it make sense to run the initial round of clustering on a smaller sample of the large dataset to select the best possible clustering algorithm and then run the selected algorithm on the entire dataset?
20:11:54 From Luis Gaxiola To Everyone:
	And how do you decide where to stop?  based on what?
20:12:36 From Daniel Gibson To Everyone:
	@Vikas, I think that this is typical with most data exploration.
20:12:44 From Ulf-2 Angelin To Everyone:
	IS there a rule of thumb when to stop clustering?
20:12:53 From Vikas Srivastava To Everyone:
	Thanks @Daniel
20:13:09 From Shel Randall To Everyone:
	You stop cooking when the omelet is done.
20:13:13 From Herman Gothe To Everyone:
	units of measure?
20:13:46 From Moderator - Ankit Agrawal To Everyone:
	Q: Would it make sense to run the initial round of clustering on a smaller sample of the large dataset to select the best possible clustering algorithm and then run the selected algorithm on the entire dataset?	A: Clustering can change with new data so running on a sample of data makes no sense.
20:14:05 From Herman Gothe To Everyone:
	center of mass?
20:14:08 From Daniel Gibson To Everyone:
	This is the ‚Äúusing highway sign distances‚Äù when driving problem.
20:14:44 From Ketan Kamdar To Everyone:
	can we use centroid distance ?
20:14:58 From Moderator - Ankit Agrawal To Everyone:
	Q: IS there a rule of thumb when to stop clustering?	A: When we reach convergence i.e. when the cluster centers don't change between iterations
20:15:12 From Ryan Salazar To All Panelists:
	Have jump early. Thanks so much Professor
20:15:47 From Thierry Azalbert To Everyone:
	on right : average, on left minium = single
20:16:03 From Mauricio Martinez To All Panelists:
	What is the center of mass?
20:16:12 From Shannon Perez To Everyone:
	Single linkage
20:16:28 From Jens M√ºller To Everyone:
	Single linkage
20:16:50 From Moderator - Ankit Agrawal To Everyone:
	Q: You can't use this methods to complement each other because of the cost?	A: Each clustering algorithm is computationally expensive i.e. takes a lot of resources to run. Running several of these methods is very hard to do as the size of data increases.
20:16:59 From Shel Randall To Everyone:
	cluster shaped like a fiber
20:17:16 From Anil Goud To Everyone:
	Avg, min, max?
20:17:20 From Ketan Kamdar To Everyone:
	single
20:17:21 From Awad Alomari,PE To Everyone:
	Single
20:17:28 From Leandro Mbarak To Everyone:
	complete
20:17:30 From Paula Valverde To Everyone:
	average
20:17:32 From Matthew Tramel To Everyone:
	complete
20:17:33 From Varuni Rao To Everyone:
	complete
20:17:33 From Angelos Calogirou To All Panelists:
	the middle one is minimum distance
20:17:34 From Megha Sharma To Everyone:
	single
20:17:37 From Mayda Alkhaldi To Everyone:
	complete
20:17:37 From Dom Lazara To Everyone:
	complete linkage
20:17:37 From Thiago Barros To Everyone:
	single
20:17:39 From EUNICE HERNANDEZ To Everyone:
	complete
20:17:47 From Bryan Olivera Lona To All Panelists:
	Single
20:18:41 From Julie Mann To Everyone:
	What if the apex of the hierarchical is chosen incorrectly or via bias.  Doesn't this initial selection make the outcome of hierarchical clustering more subject to bias than other methods?
20:19:03 From David Craig To Everyone:
	How is the phasal hierarchy represented in this model?
20:19:08 From Ketan Kamdar To Everyone:
	average
20:19:10 From Anil Goud To Everyone:
	avg
20:19:11 From Peter Ohmes To Everyone:
	Average
20:19:11 From Marcin Ladowski To All Panelists:
	average
20:19:12 From Bryan Olivera Lona To All Panelists:
	Complete
20:19:12 From Varuni Rao To Everyone:
	average
20:19:12 From Petrino Ippolito To Everyone:
	Average
20:19:12 From Arturo Gudi√±o Chong To Everyone:
	average
20:19:14 From Awad Alomari,PE To Everyone:
	average
20:19:15 From Paula Valverde To Everyone:
	average
20:19:15 From Oumar Ndiaye To All Panelists:
	Average
20:19:16 From Roman Neuhauser To Everyone:
	average
20:19:18 From Angelos Calogirou To All Panelists:
	The left one is maximum distance
20:19:18 From ankita srivastava To Everyone:
	Average
20:19:19 From Thiago Barros To Everyone:
	complete
20:19:21 From shikha sharma To All Panelists:
	maximum
20:19:22 From Marcin Ladowski To All Panelists:
	üôÇ
20:19:27 From Tripureswar Chattopadhaya To Everyone:
	average
20:19:29 From Shannon Perez To Everyone:
	Complete
20:19:30 From shikha sharma To Everyone:
	maximum
20:19:32 From Megha Sharma To Everyone:
	complete
20:19:58 From Luis Gaxiola To Everyone:
	Can you provide examples of when to use each, in which real situations?
20:20:41 From Adithya Parvatam To Everyone:
	Average linkage gives less clusters ?
20:20:48 From abdramane bathily To Everyone:
	when do you choose one or another ?
20:21:12 From Vikas Srivastava To Everyone:
	@Ankit - please explain your ‚Äúmakes no sense‚Äù comment. I understand that you will be getting new data. However, in my mind at least, you could determine how the new data fits the previously selected clusters instead of redoing the clustering. You may have to redo some clusters but I don‚Äôt think you will have to redo all the clusters.
20:21:40 From Santiago Arroyo To Everyone:
	Is this a clustering method or the parameter used to stop the K-means algorithm or other algorithms?
20:22:22 From Jeremy Boccabello To Everyone:
	Important to realize that these are 3 different data sets for which these parameters are each better suited
20:22:33 From Shannon Perez To Everyone:
	Once you have used many of them how do you determine which one is the best?
20:22:40 From Soumyo Banerjee To Everyone:
	Is there a model for comparing the output of the clustering methods to help us decide to choose the most appropriate cluster?
20:22:40 From Wilson Castiblanco Quintero To All Panelists:
	If I got a mixture of linkage, the process would need more iterations?
20:22:53 From Wilson Castiblanco Quintero To All Panelists:
	Is it possible to have a mixture of linkage ?
20:23:40 From Shel Randall To Everyone:
	Usually when given a project, you have a goal in mind. You will select methods that are better at finding the answers your specifically looking for.
20:24:12 From Shel Randall To Everyone:
	Your goals are very very rarely open ended
20:24:30 From Julio Rojas To Everyone:
	is this the basis for Ramdom Forest ML
20:24:45 From Lalit Vyas To Everyone:
	@ankit - how to apply clustering to data present in a relational database with lot of transactional data in several tables ? How to aggregate this without losing patterns? Im stuck there!
20:24:46 From Soumyo Banerjee To Everyone:
	@shel - it has a very high risk that the data scientist introduces inaccuracy or bias right from the start
20:25:38 From David Craig To Everyone:
	Does a Python output gives us a cluster dendrogram - so we can now STOP!?
20:25:46 From EUNICE HERNANDEZ To Everyone:
	Do you always choose visually?
20:25:53 From Joshua Coleman To Everyone:
	I think Vikas' question is can you start to see similar clustering in 'samples' of the full data set.  As you increase the 'sample' size will your clustering approach the result of performing the analysis on the full data set.
20:25:56 From Shel Randall To Everyone:
	@Soumyo - very true. but your project requirements are already enforcing an agenda
20:26:17 From EUNICE HERNANDEZ To Everyone:
	Sodo you always choose visually?
20:26:23 From Anil Goud To Everyone:
	So 3?
20:27:17 From Vikas Srivastava To Everyone:
	Thank you Joshua, yes that‚Äôs what I was thinking
20:28:37 From Daniel Landeros To Everyone:
	Is it generally best practice to start with PCA and t-SNE methods with multi-dimensional data and then move on to other methods based on PCA/t-SNE analysis? Or does it depend entirely on the dataset and the objectives of the analysis?
20:29:04 From fatimah alakeel To Everyone:
	good question
20:29:20 From Prashant Bapuji To Everyone:
	what is N - a non core point?
20:29:42 From Michael Miller To Everyone:
	Would is be wise to try a range of multiple cluster sizes to see what fits best to determine the ideal # of clusters to use ?
20:29:54 From Moderator - Ankit Agrawal To Everyone:
	@vikas, @Joshua: Thank you for clarifying that. Picking a sample the represents the overall data in terms of clustering similarity without actually performing the clustering is very difficult task. I haven't seen that in practice much.
20:31:40 From Alisher Jardemaliyev To Everyone:
	do i need to know all these mathematical formulas if i will only use python functions?
20:31:48 From Moderator - Ankit Agrawal To Everyone:
	Q: Is it generally best practice to start with PCA and t-SNE methods with multi-dimensional data and then move on to other methods based on PCA/t-SNE analysis? Or does it depend entirely on the dataset and the objectives of the analysis?	A: It depends on the objective. There are several other dimensionality reduction methods (over 15 of them, not just PCA or t-SNE) and then there are over 10 clustering methods. Picking the right combinations change with respect to the objective
20:31:56 From Vidhya Shankarraman To Everyone:
	In all these methods shown the no.of clusters eventually always ended up as 3, as in same inference. Is that a possibility with real large data or can this final inference change based on clustering method?
20:32:20 From gilberto scotacci To Everyone:
	@Daniel Landeros since clustering algortihm works well with a few variable dimensionality reduction algorithm are used in clustering  when we have alot of variable. However if the variables have the same meaning its better to perform an expertise selection of features
20:32:53 From Moderator - Ankit Agrawal To Everyone:
	Q: Would is be wise to try a range of multiple cluster sizes to see what fits best to determine the ideal # of clusters to use ?	A: Some clustering methods like k-means, PAM, GMM, Hierarchical are literally doing that.
20:32:53 From Joshua Coleman To Everyone:
	@Vikas, the answer should be yes.  In CFD, comparing simpler or 'subset' results to higher fidelity models is referred to as a 'sensitivity analysis'.  Your required model fidelity is such that further data points/samples/complexity does not significantly change your results.
20:32:55 From Gaetano Caolo To All Panelists:
	What for silhouette plot is used?
20:32:59 From drashti darji To Everyone:
	Do we need to remember the formula to perform this model or python will perform ?
20:33:51 From Soumyo Banerjee To Everyone:
	What is n=30?
20:33:58 From Shel Randall To Everyone:
	@Drashti - I sure hope Python will do it ‚Ä¶ I'm not going to remember this.
20:33:58 From Thiago Barros To Everyone:
	we want a to be smaller and b to be bigger, right?
20:34:05 From Lalit Vyas To Everyone:
	30 points
20:34:06 From Johnson Oguntuase To Everyone:
	Can you clarify what the functions "a" and "b" are? I assume the function "S" is the Silhouette function. Is that correct?
20:34:09 From Hugo Escandon To Everyone:
	number of points =30 samples
20:34:17 From Eric White To Everyone:
	so the black dot is 0.48.  not a good cluster?  what do we do with that then?
20:34:24 From Alisher Jardemaliyev To Everyone:
	like i think i dont completely understand the premise behind cdf or ppf, but i still use them to calculate probability :/
20:34:32 From Santiago Arroyo To Everyone:
	For b(x) the distance to the closest cluster it does not belong to is measured with respect to what point in that cluster? The centroid?
20:34:34 From Alisher Jardemaliyev To Everyone:
	in python
20:34:39 From drashti darji To Everyone:
	@shel off course , not gonna remember this big formulas for all models , never.
20:35:18 From Srinivas Annam To Everyone:
	is it practical to draw a silhouette plot for large datasets?
20:35:19 From Roman Neuhauser To Everyone:
	what is if you have too many datapoints?
20:35:43 From Melissa Ornelas To Everyone:
	there are samples imposible to cluster?
20:35:47 From drashti darji To Everyone:
	@alisher , cdf is use to calculate the cumulative probability which means you can find the more than or less than numbers and ppf is use to calculcate the percentile numbers.
20:36:09 From Anil Goud To Everyone:
	What clustering method does this silhouette plot represent? The DBScan?
20:36:23 From Daniel Gibson To Everyone:
	@Melissa, yes if there are no true subgroups or if you have not measured the variables that truly separate them.
20:36:23 From Shel Randall To Everyone:
	@Melissa - I think a perfect grid would be impossible to cluster.
20:36:27 From Mackenzie Hunt To Everyone:
	What is a bad silhouette score? Is there a standard cut off?
20:36:42 From Daniel Gibson To Everyone:
	0.5
20:36:53 From Wilson Castiblanco Quintero To All Panelists:
	Should the model run again if there are a significant amount of bad scored elements?
20:36:58 From Mackenzie Hunt To Everyone:
	Makes sense thanks!
20:37:00 From Anil Goud To Everyone:
	So you make multiple silhouette plots for each type of method you run and then choose the one that has the highest average silhouette width above 0.5?
20:37:28 From Saritha Patrick To All Panelists:
	Could the same individual condition for 9 be looked at for 7 as well to improve the clustering
20:37:54 From Michael Miller To Everyone:
	Is this software that automates this process processes ? Like APIs we can call with our data that tries these different approaches and makes judgements (suggestions) to implement ?
20:38:09 From Daniel Gibson To Everyone:
	@Anil, you don‚Äôt likely need to plot it, just calculate the average silhouette score.
20:38:22 From Omar Alsaid Sulaiman To Everyone:
	@Anil that does sound like a method to find the best alg every time, but computation power will be a big price
20:38:23 From Vikas Srivastava To Everyone:
	I kind of agree @Joshua. I was relating today‚Äôs lecture to the work I used to do a couple of decades back :-) while analyzing satellite imagery. Think of it as pictures taken from satellites. The core philosophy of the clustering (actually classification) was that everything not the planet a unique spectral signature (everything on the surface of the earth reflects or obsorbs light in a unique way). This philosophy essentially meant that if you saw anything new you would try to figure out how similar it was to other things that you have already identified.
20:38:33 From Johnson Oguntuase To Everyone:
	What do the numbers on the RHS of the plots imply?
20:38:38 From Julio Rojas To Everyone:
	how we identify what a cluster mean?
20:39:01 From Shel Randall To Everyone:
	@Vikas - sounds like a cool application
20:39:26 From Anil Goud To Everyone:
	@Omar thank you - I was hoping silhouette could intake outputs from all different clustering methods at once and then tell us which method is best, but seems alternatively you have to call the silhouette scores of each method and then compare scores
20:39:26 From Johannes Oberhofer Lomeli To Everyone:
	@ Johnson, number of element sin group and silhouette width
20:39:32 From Khang Nguyen To Everyone:
	this method is only used when you have unsupervised data right ?
20:39:57 From Johnson Oguntuase To Everyone:
	What do the numbers on the RHS of the plots imply?	Page 13: 	1: 9 | 0.48	2: 11|0.58	3: 10|0.77
20:40:01 From Lev Sukherman To All Panelists:
	Can we use UMAP, because it is a method dimensionality reduction and clustering?
20:40:09 From Daniel Gibson To Everyone:
	@Mike depends, are you doing basic science, or using data to guide actions (applied science)?
20:40:16 From Vikas Srivastava To Everyone:
	Yeah @shel its pretty cool
20:40:26 From shikha sharma To Everyone:
	single
20:40:34 From Wilson Castiblanco Quintero To All Panelists:
	Choosing a wrong clustering is tied to the wrong use of SNE or PCI?
20:40:37 From Omar Alsaid Sulaiman To Everyone:
	@Anil you could define a function to loop over all methods, but you'll need to get a second job to pay that electricity bill
20:40:38 From Jakub Baranowski To Everyone:
	Hierarchial w/ single linkage and prolly DBSCAN
20:40:39 From Bryan Olivera Lona To All Panelists:
	Dbscan
20:40:39 From Luca Mastrantoni To Everyone:
	DBSCAN would work, K.means no
20:40:41 From Peter Ohmes To Everyone:
	hierarchical - single
20:40:43 From Lamine Ndiaye To Everyone:
	Single
20:40:46 From Daniel Gibson To Everyone:
	DBSCAN
20:40:47 From Sellathurai Sriganesha To Everyone:
	K-means clustering
20:40:49 From Paula Valverde To Everyone:
	Hierarchical single
20:40:50 From Shannon Perez To Everyone:
	Dbscan
20:40:50 From Sharon Kuang To Everyone:
	dbscan
20:40:53 From Ketan Kamdar To Everyone:
	k-means
20:40:53 From Adithya Parvatam To Everyone:
	guassian
20:40:54 From Viviana Gonzalez To All Panelists:
	Single
20:40:54 From Saritha Patrick To All Panelists:
	single - linkage
20:40:55 From Leandro Mbarak To Everyone:
	DBSCAN
20:40:55 From Varuni Rao To Everyone:
	DBSCAN
20:40:56 From Hugo Escandon To Everyone:
	simple linkage
20:40:57 From Jens M√ºller To Everyone:
	K mean
20:41:00 From Ulf-2 Angelin To Everyone:
	dbscan
20:41:04 From Adithya Parvatam To Everyone:
	Single linkage
20:41:42 From Jens M√ºller To Everyone:
	Ah okay‚Ä¶
20:41:51 From Daniel Gibson To Everyone:
	Reviving an older question.  If these points were in radial/spherical coordinates, would these methods work differently?
20:41:57 From EUNICE HERNANDEZ To Everyone:
	What is an example in real life when you would see those clusters?
20:42:29 From Michael Miller To Everyone:
	@Daniel, I'm a database programmer (not data analysis guru), but I look for ways to automate repetitive tasks - I assume there must be libraries doing this already
20:42:34 From Lev Sukherman To All Panelists:
	Can we use ANN to solve this problem?
20:42:42 From Petrino Ippolito To Everyone:
	When you have many dimensions and cannot visualize the data, how would you know you have a case like this to decide on this method?
20:42:47 From Joshua Coleman To Everyone:
	@Vikas, makes sense.  At the end of the day, a main point of clustering is to predict or leverage against future data.  Adding new points will increase accuracy and will require new clustering - however it may negligibly increase the accuracy.  There should be intermediate points of 'sufficient' accuracy
20:43:12 From Leandro Mbarak To Everyone:
	using Silhouette plot we can identify if we have an error like this, because this kind of visualization is not always possible.
20:43:18 From drashti darji To Everyone:
	Q- is the single linkage applied on individual AggLabels or it can interact each other ? As it says it will find minimum distance.
20:43:25 From owen sanford To Everyone:
	so, the answer was either single linkage OR DBScan?
20:43:34 From Daniel Gibson To Everyone:
	@Owen, yes
20:43:34 From Vikas Srivastava To Everyone:
	Yes @Joshua
20:43:38 From Shel Randall To Everyone:
	I wish I had a decision tree flowchart sequence for figuring how/when to use all these tools. (I think I need and AI to help me figure out how to build an AI.) :)
20:44:08 From Anil Goud To Everyone:
	There is probably something out the web regarding that already
20:44:09 From Adithya Parvatam To Everyone:
	Gaussian vs dbscan  ..please
20:44:17 From owen sanford To Everyone:
	@daniel--thanks!
20:44:24 From Shel Randall To Everyone:
	@Anil - I will certainly be looking for that!
20:44:28 From ankita srivastava To Everyone:
	https://chat.whatsapp.com/L3UiClsqljYAq7CjMe7667
20:44:31 From Anil Goud To Everyone:
	Let me know!
20:44:36 From Herman Gothe To Everyone:
	I have the doubt if first is necessary normalize the data?
20:44:37 From Dom Lazara To Everyone:
	what would tSNE find for this data?
20:44:37 From Omar Alsaid Sulaiman To Everyone:
	@Shel there are few good posts (decision trees) to help which ML alg to use on LinkedIn
20:45:14 From Saritha Patrick To All Panelists:
	what is the best way to combine two clusters to identify a dataset
20:45:14 From Wilson Castiblanco Quintero To All Panelists:
	Choosing a wrong clustering is tied to the wrong use of SNE or PCI?
20:45:18 From Julie Mann To Everyone:
	please explain why the case study does not include kmeans with the linear boundaries defined
20:45:39 From Shel Randall To Everyone:
	@Omar - we should post those in our Slack chat or something
20:45:41 From O GO To All Panelists:
	Whats the similarities and differences of Networks and Neural Networks?
20:45:43 From Herman Gothe To Everyone:
	It is necessary first normalize the data.
20:45:53 From Soumyo Banerjee To Everyone:
	From  https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
20:45:54 From David Enck To Everyone:
	@Shel, I just asked the Bing AI search to build a decision tree and it did. looked reasonable
20:46:15 From Omar Alsaid Sulaiman To Everyone:
	@David share it
20:46:29 From Shel Randall To Everyone:
	So, you used an AI to help you build an AI ? lol
20:46:47 From Saritha Patrick To All Panelists:
	lol
20:47:48 From Omar Alsaid Sulaiman To Everyone:
	@Soumyo thanks, that seems really helpful
20:47:57 From Joshua Coleman To Everyone:
	@Julie its point to point distance, not an overall grouping distance - so you'll end up with a line versus a curve separating cluster nodes
20:48:04 From Alisher Jardemaliyev To Everyone:
	can you use some model to find best clustering model for your dataset?
20:48:43 From alberto chico To Everyone:
	how are neurons represented or its structure in neural networks?
20:49:01 From David Craig To Everyone:
	Thank you Ankita - that diagram is brilliant! British idiom-speak for Awesome!
20:49:33 From David Enck To Everyone:
	change a few words and the outcome changes, of course
20:49:48 From David Enck To Everyone:
	Request: please build a decision tree for determining which cluster method to use
20:50:43 From Joshua Coleman To Everyone:
	@David check out the link Soumyo provided above
20:51:09 From Judith Marrugo To Everyone:
	David maybe is the work of a Data Scientist !
20:51:23 From Petrino Ippolito To Everyone:
	Oh yeah that chart is solid, thank you @souymo
20:51:33 From David Enck To Everyone:
	Thank you!
20:52:02 From Joshua Coleman To Everyone:
	I like "Get More Data" branch haha
20:52:21 From Moderator - Ankit Agrawal To Everyone:
	Q: how are neurons represented or its structure in neural networks?	A: There will be a dedicated week for Neural networks a few weeks from now
20:52:33 From Daniel Gibson To Everyone:
	@Joshua, likely the most common answer, with the retort being, ‚Äúbut we need more money‚Äù.
20:53:23 From Joshua Coleman To Everyone:
	no the answer is usually a question: can't we get an answer without data?
20:53:44 From Moderator - Ankit Agrawal To Everyone:
	Q: Whats the similarities and differences of Networks and Neural Networks?	A: Networks represent graph data (it is a data structure). Neural network is a deep learning algorithm that can learn patterns in data (including graph data)
20:53:46 From Daniel Gibson To Everyone:
	True, ‚Äústay the course‚Äù is easy.
20:53:59 From Raj Desikavinayagompillai To Everyone:
	Politicians classifying voters into clusters to find the shortest path to reach the voters efficiently reach their political message
20:54:59 From Varuni Rao To Everyone:
	sounds like hierarchical
20:55:08 From Shel Randall To Everyone:
	"Yes, Boss. I will use statistical analysis to get your results." (Rolls dice)
20:55:14 From Alisher Jardemaliyev To Everyone:
	we break up edges with them because they can destroy network and therefore destroying clusters?
20:55:19 From Sujasha Gupta To Everyone:
	How to know when to stop removing edges
20:55:35 From Daniel Gibson To Everyone:
	@Shel nice montecarlo model!
20:56:01 From Shel Randall To Everyone:
	lol
20:56:05 From David Craig To Everyone:
	Is this named for the university in Belgium?
20:56:40 From Fidel Vargas To Everyone:
	What is the difference betwwen the Louvain method and the Leiden method
20:56:47 From Fidel Vargas To Everyone:
	between
20:57:25 From Varuni Rao To Everyone:
	both come up with tree structures for the networks?
20:57:57 From Julie Mann To Everyone:
	So is Louvain method like: being more connected to your work wife than your wife?
20:58:05 From Thierry Azalbert To Everyone:
	Careful, in Belgium French and FLEMISH are spoken, not Dutch!
20:58:16 From Joshua Coleman To Everyone:
	lol Julie
20:58:35 From Mayda Alkhaldi To Everyone:
	What does the distance between each node tell us here?
20:58:37 From Alisher Jardemaliyev To Everyone:
	Flemish is dialect of Dutch lol
20:59:16 From Shel Randall To Everyone:
	I'm picturing galaxies, galaxy groups and clusters, superclusters
21:00:13 From EUNICE HERNANDEZ To Everyone:
	Can the size of the dots be seen as other smaller clusters?
21:00:43 From Kalyan Gorrepati To Everyone:
	Is this image an output of some process to visualize ?
21:00:51 From Louis Reid To Everyone:
	Thank you!!!
21:01:00 From owen sanford To Everyone:
	thank you.
21:01:04 From Jason Cuevas To Everyone:
	Thank you
21:01:05 From Tatiana A To Everyone:
	Thank you :)
21:01:07 From Peggy Hsieh To All Panelists:
	amazing! thank you!
21:01:09 From Tracy Katz To All Panelists:
	Thank you!!
21:01:10 From Thiago Barros To Everyone:
	Thank you Prof. Caroline!
21:01:11 From Frances Rangel To Everyone:
	Thank you
21:01:12 From Sellathurai Sriganesha To Everyone:
	Thank you
21:01:12 From Adriano Oliveira To Everyone:
	Thank you!
21:01:15 From Yadira M Del Rio To All Panelists:
	Thank you, great lecture!
21:01:17 From Juan Ciszek To All Panelists:
	Thanks
21:01:18 From Bala Kumaaran Gopalakrishnan To Everyone:
	Thank You !
21:01:24 From Sravan Muthyam To All Panelists:
	Thank you!
21:01:33 From Erwin Iost To Everyone:
	Thanks!!!
21:01:33 From shikha sharma To Everyone:
	Thank you
21:01:33 From Varuni Rao To Everyone:
	Thank you Dr. Uhler
21:01:33 From Srinivas Annam To Everyone:
	Thank you!
21:01:33 From Julie Mann To Everyone:
	Thank you, Prof Caroline
21:01:33 From Jakub Baranowski To Everyone:
	Thank you!
21:01:33 From Michael Miller To Everyone:
	Thank You Professor
21:01:34 From Shel Randall To Everyone:
	Please have all those books read by next week.
21:01:34 From Chloe Liban To Everyone:
	thank you !
21:01:34 From Sharon Kuang To Everyone:
	Thank you
21:01:36 From Ulf-2 Angelin To Everyone:
	Thanks Professor, great sessions!
21:01:36 From Seher Rehan To Everyone:
	Thankyou
21:01:36 From Roman Neuhauser To Everyone:
	thank you very much
21:01:39 From Iwan M√ºller To Everyone:
	Thank you
21:01:39 From Vidhya Shankarraman To Everyone:
	Thank you Prof. Uhler!!
21:01:39 From Luis Mora To All Panelists:
	Thank you!
21:01:39 From Angelos Calogirou To All Panelists:
	Actually, there is even a tiny German speaking minority in Belgium. I guess it disappears in such a Clustering process
21:01:39 From Arturo Gudi√±o Chong To Everyone:
	Thank you! Prof. Caroline
21:01:40 From Csaba Tamas To All Panelists:
	Thank you
21:01:41 From Mikael Friederich To Everyone:
	Awesome, thanks
21:01:42 From Anil Goud To Everyone:
	Thank you Professor Uhler!!!!!! You are awesome
21:01:42 From Han H To Everyone:
	Thank you!
21:01:45 From Mackenzie Hunt To Everyone:
	Thank you very much!
21:01:45 From Adithya Parvatam To Everyone:
	Thank you professor
21:01:45 From Paula Valverde To Everyone:
	Thank you for your insightful lectures! I enjoyed them a lot
21:01:45 From David Enck To Everyone:
	Thank you Professor!
21:01:46 From Jose Rojas To Everyone:
	Thanks Prof. Uhler!
21:01:46 From Zainab Saccal To Everyone:
	Thank you
21:01:47 From Tanya Makeev To Everyone:
	Thank you!
21:01:48 From Sarah Cleve To All Panelists:
	Thank you
21:01:48 From Awad Alomari,PE To Everyone:
	Thank You , Prof.
21:01:50 From Prashant Bapuji To Everyone:
	Thank you Prof !!
21:01:51 From David Craig To Everyone:
	Thank you Prof. Uhler!
21:01:52 From Yogesh Kadam To Everyone:
	Thank you Prof.!!!
21:01:53 From Tripureswar Chattopadhaya To Everyone:
	Thank you Professor .
21:01:53 From Gaetano Caolo To All Panelists:
	Thank you very much.
21:01:57 From Megha Sharma To Everyone:
	Thanks Professor
21:01:57 From abdramane bathily To Everyone:
	thank you
21:01:58 From Michael Wahnich To Everyone:
	Danke Schon
21:02:00 From Mike D. To Everyone:
	thank you good luck for your career. Tchuss!
21:02:05 From Jens M√ºller To Everyone:
	Thank you very much. It was very comprehensive. I really like your style of teaching/presenting :-))
21:02:05 From Johannes Oberhofer Lomeli To Everyone:
	Thank you Professor Uhler. Great lecture!!
21:02:12 From Aman Singhai To Everyone:
	Thank you Professor!
21:02:12 From Santiago Arroyo To Everyone:
	Thank you!
21:02:12 From lynn liang To Everyone:
	Thank you Prof!
21:02:14 From Sujasha Gupta To Everyone:
	Thank you professor
21:02:20 From Ratna Dhavala To Everyone:
	Thank you
21:02:21 From Marcin Ladowski To Everyone:
	Thank you Prof.  Uhler. Great sessions!
21:02:21 From Sumeet Deshpande To Everyone:
	Thank You, Professor Uhler !!!
21:02:21 From Tatiana A To Everyone:
	https://web.stanford.edu/class/cs168/1/111.pdf *Link no longer works
21:02:23 From Stephany Gochuico To Everyone:
	Thank you Prof. Uhler !
21:02:23 From Barbara Timm-Brock To Everyone:
	Thank you, your lectures have been interesting!
21:02:23 From Thierry Azalbert To Everyone:
	Thank you for a most informative series of lectures!
21:02:23 From EUNICE HERNANDEZ To Everyone:
	Thanks Prof. Caroline, very insightful lessons!
21:02:24 From Julio Rojas To Everyone:
	Thanks a lot, as all lectures this week, very good ones
21:02:26 From Dom Lazara To Everyone:
	Thank you Professor
21:02:27 From Mario Lemos To All Panelists:
	Thank professor, thank you all, great lecture
21:02:28 From Lalit Vyas To Everyone:
	Thanks Prof. Uhler!
21:02:28 From Lita Miranda To Everyone:
	Thank you, Professor Caroline!
21:02:28 From Joshua Coleman To Everyone:
	Thank you!
21:02:30 From Ben Germany To Everyone:
	Thank you!!!
21:02:32 From Petrino Ippolito To Everyone:
	Thanks!
21:02:33 From melanie rebosa To All Panelists:
	Thank you!
21:02:36 From Daniel Bautista To Everyone:
	Thank you
21:02:37 From Himansu Jena To Everyone:
	Thank you!
21:02:37 From Azaria Berhane To Everyone:
	Thank you Professor
21:02:38 From Gabriel Signorelli To Everyone:
	thanks
21:02:38 From Rishi Khanna To Everyone:
	Thanks
21:02:39 From Vikas Srivastava To Everyone:
	Thank you!
21:02:40 From Neha Purohit To All Panelists:
	Thank you
21:02:43 From Bhupal Lambodhar To All Panelists:
	Thank you
21:02:46 From Mahesh Ballani To Everyone:
	thank you!
21:02:46 From Jeremy Boccabello To Everyone:
	Thank you very much.  This was very interesting and a great way to recap all of the case studies and answering many questions that arose during them.
21:02:46 From Fernando Matias Gonzalez To Everyone:
	Great sessions, thank you very much!
21:02:47 From Carlos Eduardo Jauregui Briceno To Everyone:
	Thanks
21:02:47 From Judith Marrugo To Everyone:
	Thans Caroline !!!
21:02:48 From Helena Monteiro To Everyone:
	Thnak you.
21:02:49 From Wilson Castiblanco Quintero To All Panelists:
	Thank you Dr Caroline!
21:02:49 From Samuel Wright To Everyone:
	Thank you all
21:02:49 From Leandro Mbarak To Everyone:
	thank you
21:02:51 From Vikas Srivastava To Everyone:
	Interesting content for sure
21:02:51 From Bernardo Serafin Millan To Everyone:
	Thank you so much Professor
21:02:52 From Jaime Romanini To Everyone:
	The Stanford reference link is broken
21:02:52 From Matthew Tramel To Everyone:
	Thank you Professor!
21:02:52 From Shel Randall To Everyone:
	ty
21:02:52 From Cristobal Fresno To Everyone:
	Thank you
21:02:54 From Fernando Schmidkonz To Everyone:
	Great session Professor!
21:03:00 From Mauricio Maldonado To Everyone:
	Great lecture, thank you!
21:03:01 From Marisol Santillan To Everyone:
	Thank you!
21:03:06 From drashti darji To Everyone:
	Thank You, Have a nice day Prof.
21:03:06 From Viviana Gonzalez To All Panelists:
	Thank you!
21:03:07 From Jaime Romanini To Everyone:
	Thanks you so much
21:03:11 From Bryan Olivera Lona To All Panelists:
	Thank you, Prof!
21:03:17 From Shel Randall To Everyone:
	What time zone is Prof. Uhler in?
21:03:20 From Tripureswar Chattopadhaya To Everyone:
	Can you please showcase some Data and discuss the same
21:03:35 From Johannes Oberhofer Lomeli To Everyone:
	I have to leave today. Will check recording of this part later.
21:03:43 From Herman Gothe To Everyone:
	Please could you explain if it is needed to normalize the data first?
21:03:43 From Alisher Jardemaliyev To Everyone:
	I have a question regarding Algorithm of Givan and Newman, why do you need to remove edges with highest betweenes centrality?
21:03:48 From Bryan Olivera Lona To All Panelists:
	Q: Is there a single Python package with all of these clustering methods
21:03:52 From Bryan Olivera Lona To All Panelists:
	?
21:03:53 From Mayda Alkhaldi To Everyone:
	I have a question about the slide from the lecture about the example of French and dutch speaking individuals. What does the distance between each node mean or tell us?
21:04:04 From drashti darji To Everyone:
	Need to leave, have a nice day!
21:04:13 From Shel Randall To Everyone:
	French + Dutch = Frutch? Dench?
21:04:20 From Soumyo Banerjee To Everyone:
	The concepts covered today were too abstract - we never went into detailed case study‚Ä¶
21:04:48 From Jaime Romanini To Everyone:
	Do you have and example in the greatlearning site? i could not find any
21:04:52 From Samuel Wright To Everyone:
	I will not be able to stay for the Q&A session today.
21:04:52 From Fausto Correa To Everyone:
	Thanks for the lecture, very interesting.
21:04:54 From Azaria Berhane To Everyone:
	Where are best places to see a real world uses examples of these strategies to get a better grasp of these theories?
21:04:56 From Jakub Baranowski To Everyone:
	@Alisher removing nodes with highest betweenes score has the highest chance of breaking the network into separate parts.
21:04:57 From Shel Randall To Everyone:
	Soumyo - there are some case studies in the course material ‚Ä¶ which I hope will help
21:05:19 From Soumyo Banerjee To Everyone:
	Yep - weekend home work‚Ä¶
21:05:38 From Pedro Ju√°rez Lagos To Everyone:
	Thank you!
21:05:39 From Shel Randall To Everyone:
	yeah I had no plans at all for this weekend ‚Ä¶ apparently. :)
21:05:42 From Herman Gothe To Everyone:
	Please could you explain if it is needed to normalize the data before clustering?
21:06:02 From Bryan Olivera Lona To All Panelists:
	I have to drop off the call, thank you Ankit and Niruppam!
21:06:11 From Lev Sukherman To Everyone:
	Could tell about cases when do we need to use t-SNE instead of UMAP?
21:06:24 From Shel Randall To Everyone:
	gtg ‚Ä¶ yall take care
21:06:27 From Lev Sukherman To Everyone:
	Can we use UMAP for clustering?
21:07:11 From Ricardo Vides To Everyone:
	regarding scaling, when should we normalization vs standarization?
21:07:26 From Merrill Kashiwabara To All Panelists:
	Would you use different clustering methods on different (shaped) clusters within the same dataset?
21:08:02 From Adithya Parvatam To Everyone:
	Professor spoke of topdown approach and bottom up approach of clustering .. can you elaborate more please
21:08:03 From Lalit Vyas To Everyone:
	Repeat Q - How do you aggregate transactional data to n-dimensional data without losing patterns? Are there any methods to identify patterns in transactional data?
21:09:44 From John Schliesmann To Everyone:
	Could you share a python script that demonstrates a Silhouette plot?
21:09:49 From David Craig To Everyone:
	The conceptual range of this week's material was enormous, fascinating, but enormous.  The Hands-on Python videos provided (e.g., Genomic Data Clustering) was also interesting.  As I  look to review, this weekend, for the upcoming test, will this test-format continue to be a mixture of (1) conceptual-range, (2) vocabulary meaning and (3) Python running code (on Google-Collab) questions?  Can you address this?
21:12:12 From Srinivas Annam To Everyone:
	+1 to David's question. In fact, as this is answered it will be helpful to get some guidance on which of these concepts are more useful practically for the long run can be useful for us to focus on the right topics.
21:12:14 From Lamine Ndiaye To Everyone:
	Please, what is the difference between medoid and centroid?
21:13:02 From Lamine Ndiaye To Everyone:
	Thank you very much!
21:14:39 From Judith Marrugo To Everyone:
	Are u going with us in weekend?
21:14:55 From Adithya Parvatam To Everyone:
	Silhouette score above 0.5
21:14:57 From Luis Gaxiola To Everyone:
	Regarding again the use of Medoid in PAM:  when you have to recalculate/update the center in the next iterations... do you move to a different data point, or this time to a centroid?
21:15:40 From Luca Mastrantoni To Everyone:
	Just a quick recap, we can use Louvain method to identify communities (which is similar to hierarchical clustering). We can also use all of the other methods (such as K-means, DBSCAN etc) to identify clusters, in this case we use these methods on centrality measures. Is this correct? Thanks
21:15:41 From Adithya Parvatam To Everyone:
	Have a good day guys
21:15:44 From Jeremy Boccabello To Everyone:
	Actionable information
21:17:03 From Kalyan Gorrepati To Everyone:
	Are there benchmarks on how big the dataset can be, whether the data should be distributed/on a single machine, power of the machines, how long clustering will take for a certain size dataset ?
21:17:13 From Thierry Azalbert To Everyone:
	@John . the is one in the Practice_Case_Study_Clustering.ipynb notebook
21:19:28 From Yasir Maqbool To Everyone:
	Yesterday, you suggested a tool for networks. can you please tell the name again, and is it applicable for clustering, too?
21:19:47 From Moderator - Ankit Agrawal To Everyone:
	Neo4j
21:21:35 From Herman Gothe To Everyone:
	Are libraries that will give you after clusterization, what are the main features that characterize a cluster?
21:22:04 From Dom Lazara To Everyone:
	In the case study (slide 17), would tSNE be able to find the two circular clusters? Or would DBSCAN or single linkage be the only way?
21:22:11 From [GL mento] Niruppam To Everyone:
	Q: Are libraries that will give you after clusterization, what are the main features that characterize a cluster? A: you do it manually with python
21:22:25 From Ricardo Vides To Everyone:
	in a business setting, what programming tools do you use to run clustering algorithm? (e.g. Spark ML, Azure ML, GCP, Amazon, desktop python run in a server, etc..) and what is the most popular clustering method used?
21:23:43 From Herman Gothe To Everyone:
	are instructions in python to find what are the common or similar characteristics of a cluster after clusterizing?
21:24:01 From Peter Ohmes To Everyone:
	So is it standard practice to use the cluster labels as filtering criteria for linear/nonlinear correlation models?
21:24:05 From Petrino Ippolito To Everyone:
	In terms of running things on your own system, what are the important specs? RAM and VRAM?
21:25:26 From Herman Gothe To Everyone:
	are instructions in python to find what are the common or similar characteristics or features of a cluster after clusterizing?
21:26:52 From Ricardo Vides To Everyone:
	do you recommend using auto ML to run clustering algorithms? or can you use it in some way to validate or complement your own script?
21:26:59 From Luis Mora To All Panelists:
	In your own projects for employers, what distance metrics have you used?
21:27:00 From David Craig To Everyone:
	Thank you Ankit and Niruppam. I found all three of your half-hour feedback-sessions , this week, very helpful.
21:27:22 From David Enck To Everyone:
	Thank you!
21:28:48 From Luis Mora To All Panelists:
	Thank you!
21:30:17 From ankita srivastava To Everyone:
	Thankyou All
21:30:45 From Moderator - Ankit Agrawal To Everyone:
	Q: What is the difference betwwen the Louvain method and the Leiden method	A: https://www.nature.com/articles/s41598-019-41695-z/
21:30:50 From Shilpa Gokhale To Everyone:
	Thank you!
21:31:14 From Parimal Khedkar To All Panelists:
	Thank you!
21:31:34 From Michael Miller To Everyone:
	Dataframes are 2 dimensional structures, how do we use dataframes for this ?
21:31:38 From Tanya Makeev To Everyone:
	Thank you!
21:31:40 From Fidel Vargas To Everyone:
	ok,thanks
21:31:40 From Valeria Herrera To All Panelists:
	Thank you
21:31:42 From Jens M√ºller To Everyone:
	Thank you :-)
21:31:47 From Ketan Kamdar To Everyone:
	Thx
21:31:50 From Vanessa Larissi To All Panelists:
	Thank you
21:31:51 From John Schliesmann To Everyone:
	Thank you!
21:31:53 From JUAN RIOS To All Panelists:
	Thanks !
21:31:53 From Thiago Barros To Everyone:
	Thank you!
21:31:59 From EUNICE HERNANDEZ To Everyone:
	Thanks Ankit and Niruppam
21:32:02 From alberto chico To Everyone:
	thank you! have a great rest of the day everyone
21:32:03 From Fernando Matias Gonzalez To Everyone:
	Many thanks
21:32:04 From Gabriel Signorelli To Everyone:
	thanks
21:32:06 From Mauricio Maldonado To Everyone:
	Thank you!!
21:32:08 From Ulf-2 Angelin To Everyone:
	Thanks guys
21:32:08 From Lalit Vyas To Everyone:
	Thanks
21:32:08 From Rishi Khanna To Everyone:
	Thanks
21:32:09 From Sophia Heller To All Panelists:
	Thank you!
21:32:09 From Merrill Kashiwabara To All Panelists:
	Thanks!
21:32:09 From Yogesh Kadam To Everyone:
	thanks
21:32:10 From Honglian GUO To Everyone:
	Thank you
21:32:13 From Megha Sharma To Everyone:
	Thanks
21:32:16 From Javier Eduardo M√°rquez Orjuela To All Panelists:
	Thanks
21:32:18 From abdramane bathily To Everyone:
	thank you
