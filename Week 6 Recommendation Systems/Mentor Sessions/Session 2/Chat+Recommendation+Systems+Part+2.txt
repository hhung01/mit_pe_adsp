19:56:16 From Tripureswar Chattopadhaya To Everyone:
	Good Evening from Saudi Arabia
19:56:57 From Rahul Kumar To Everyone:
	Good Morning All
19:57:05 From Jose Rojas To Everyone:
	Good Morning!
19:57:20 From Jason Cuevas To Everyone:
	Good Morning
19:57:24 From German Quintero To Everyone:
	Good Morning
19:57:45 From Iwan Müller To Everyone:
	Good afternoon from Switzerland
19:58:04 From Yogesh Kadam To Everyone:
	Good Morning everyone
19:58:17 From Judith Marrugo To Everyone:
	Good morning from Bogotá Colombia South America
19:58:20 From subhra roy To All Panelists:
	Good Morning everyone, I am from New Jersey , USA
19:58:35 From Mike D. To Everyone:
	Hello from Maine!
19:58:51 From anu joshi To All Panelists:
	Good morning
19:58:55 From Arturo Gudiño Chong To Everyone:
	Good Morning from Tijuana, Mexico
19:59:01 From Shel Randall To Everyone:
	Good morning from Northern California for our 2nd to last lecture! (sadness!)
19:59:13 From Megha Sharma To Everyone:
	Good Morning everyone
19:59:14 From Louis Reid To Everyone:
	Good Morning from Newport, Rhode Island!!!
19:59:14 From Mike D. To Everyone:
	sadness indeed!
19:59:25 From Fausto Correa To All Panelists:
	Good morning everyone!!!
19:59:32 From Johan Goedkoop To Everyone:
	good day
19:59:34 From Navin Kumar To Everyone:
	Good Morning from California USA
19:59:53 From Olamide Olowoyo To Everyone:
	Good morning from Queens!
20:00:00 From Julie Mann To Everyone:
	Good morning
20:00:00 From Santiago Arroyo To Everyone:
	Buenos días
20:00:07 From Mehdi Pirooznia To Everyone:
	Good morning
20:00:07 From Cristobal Fresno To Everyone:
	Good morning!!!
20:00:08 From Srihari Chelluri To All Panelists:
	Good Morning
20:00:09 From Eric Bonney To Everyone:
	Good Morning everyone
20:00:11 From Herman Gothe To Everyone:
	Good morning for Santiago, Chile
20:00:14 From Fernando Matias Gonzalez To Everyone:
	Good morning everyone
20:00:18 From Jens Müller To Everyone:
	Good afternoon from Hamburg :-)
20:00:19 From Awad Alomari To Everyone:
	Good Evening from Doha, Qatar
20:00:30 From Han H To Everyone:
	Good morning!
20:00:38 From Fredy Vigo To Everyone:
	Good morning!
20:00:48 From Raj Desikavinayagompillai To Everyone:
	Good morning from San Jose , california
20:00:51 From Pierce Ruggles To Everyone:
	Good morning from Boulder, Colorado
20:00:59 From Paula Valverde To Everyone:
	Buenas tardes from Spain (Morning! for you)
20:00:59 From Yadira M Del Rio To Everyone:
	Good morning!
20:01:08 From Lalit Vyas To Everyone:
	Good morning everyone! :)
20:01:28 From Frederik Vanaverbeke To Everyone:
	Top of the morning!
20:01:29 From Lalit Vyas To Everyone:
	From Dubai! 6:30 PM here! haha
20:01:32 From Daniel Politzer To All Panelists:
	Good morning from Brooklyn!
20:01:37 From Michael Wahnich To Everyone:
	Afternoon from London
20:01:38 From Alfred Baumbusch To Everyone:
	good morning from San Antonio
20:01:38 From Samuel Wright To Everyone:
	GM from Philadelphia
20:01:38 From Lev Sukherman To All Panelists:
	Morning from Boston!
20:01:39 From Kebron Negesse To All Panelists:
	Good morning from DC :)
20:01:41 From David Palsgrove To Everyone:
	Hello for LIC Queens
20:01:45 From Eric White To Everyone:
	Winchester, MA
20:01:49 From Alfred Baumbusch To Everyone:
	Texas
20:01:51 From Roman Neuhauser To Everyone:
	Good afternoon, from Switzerland
20:01:51 From simona tenaglia To Everyone:
	good afternoon for Rome
20:01:52 From Srihari Chelluri To All Panelists:
	Good Morning, VA
20:01:52 From Maâmar M To Everyone:
	Hi From Paris
20:01:54 From Honglian GUO To Everyone:
	Good Afternoon from Mozambique
20:01:55 From Ratna Dhavala To Everyone:
	good morning from Boston
20:01:57 From Rishi Khanna To Everyone:
	Good Morning from Seattle
20:01:58 From Fausto Correa To All Panelists:
	9:31 am from Mississauga Canada
20:02:02 From Himansu Jena To Everyone:
	Good evening  from Bengaluru
20:02:03 From owen sanford To Everyone:
	0930 in DC. In the heart of the workday monitoring 3 meetings as well:)
20:02:09 From Mario Lemos To All Panelists:
	Hi everyone, Angola
20:02:10 From Claudio Chaves To All Panelists:
	GM from Florida
20:02:16 From Shel Randall To Everyone:
	wow it's 6:30 AM here …
20:02:16 From Alfred Baumbusch To Everyone:
	God Bless Hamish Harding from Dubi
20:02:17 From Lamine Ndiaye To Everyone:
	Good afternoon from Dakar, Senegal
20:02:23 From Mahmoud Mansi To All Panelists:
	Hello from Geneva
20:02:24 From Jean Vanlanduyt To Everyone:
	good afternoon from Belgium
20:02:24 From Awad Alomari To Everyone:
	You are most welcome Professor
20:02:24 From ashish kumar To Everyone:
	Hello from Columbus Ohio
20:02:26 From Claudio Chaves To Everyone:
	GM from Florida
20:02:28 From Leandro Mbarak To Everyone:
	hello from Buenos Aires
20:02:32 From Arturo Gudiño Chong To Everyone:
	6:30 AM here
20:02:35 From Jaime Romanini To Everyone:
	Good afternoon from Chile
20:02:36 From Cheryl Danton To Everyone:
	Good morning from Chicago
20:02:41 From Yasir Maqbool To Everyone:
	Hi from Riyadh Ksa
20:02:42 From Thierry Azalbert To Everyone:
	Good evening from Kuala Lumpur
20:02:51 From David Craig To Everyone:
	Dorchester...close to your Cambridge Piza
20:03:05 From Eric White To Everyone:
	big props to the west coasters up at 6:30AM to learn recommendation systems
20:03:07 From Azaria Berhane To Everyone:
	Good Morning from Washington state
20:03:16 From Herman Gothe To Everyone:
	@Jaime Romanini saludos de un compatriota
20:03:27 From Angalar Chi To Everyone:
	Good Morning from Bay Area, California
20:03:28 From Ali Malçok To Everyone:
	Good morning from Cape cod :)
20:03:28 From Mayda Alkhaldi To Everyone:
	Morning from Toronto
20:03:31 From Michael Miller To Everyone:
	Good Morning from Charlotte, North Carolina
20:03:35 From Stephany Gochuico To Everyone:
	Hello from Paris, France
20:03:36 From Olamide Olowoyo To Everyone:
	Congrats to him!
20:03:55 From Mauricio Maldonado To Everyone:
	Good morning from Guadalajara Mexico!
20:03:57 From Dom Lazara To Everyone:
	A quote to start the day ... On selecting a restaurant: "Nobody goes there anymore. It's too crowded." - Yogi Bera
20:04:15 From Bhupal Lambodhar To Everyone:
	Good Morning Everyone from San Diego
20:04:37 From Abdramane Bathily To Everyone:
	Hi everyone
20:07:03 From simona tenaglia To Everyone:
	from yesterday lesson, why we choose a probability function deriving from flipping a coin? could we choose another type of prob function?
20:08:16 From Cristobal Fresno To Everyone:
	Why 0.75?
20:08:27 From Cristobal Fresno To Everyone:
	Thank you
20:08:44 From Judith Marrugo To Everyone:
	Thanks Cristóbal for asking
20:08:47 From Shel Randall To Everyone:
	@Simona - "what card did you pick?", "what dice did you roll?" "Did my kids flush the toilet?" … all completely random
20:09:03 From simona tenaglia To Everyone:
	ok thanks
20:10:54 From Judith Marrugo To Everyone:
	Could u repeat the steps please?
20:11:04 From Ricardo Vides To Everyone:
	can you provide an example of retail recommendation systems? and how to deal with bias due to purchases that come from gift or any other purchase that don't represent the customer preference?
20:14:07 From Varuni Rao To Everyone:
	Q: To determine the community, we need to cluster them on similarity?
20:14:32 From Shel Randall To Everyone:
	Model becomes less reliable at the fringes … ?
20:14:40 From Varuni Rao To Everyone:
	ok. Thank you.
20:15:17 From Judith Marrugo To Everyone:
	Thanks profesor it is coming more clear
20:16:12 From Raj Desikavinayagompillai To Everyone:
	Distance measure of the community will provide similarity scores for the communities?
20:16:12 From Shel Randall To Everyone:
	I dated Taylor Swift .. through 4 other people.
20:17:29 From Raj Desikavinayagompillai To Everyone:
	Dating algorithm to recommend a match/ room mate matching for a dorm based on interest
20:18:12 From Shel Randall To Everyone:
	@Raj - I want that system to pick a doctor for me that's not a jerk.
20:18:42 From Raj Desikavinayagompillai To Everyone:
	Depends on where u went last time and what feedbacks u have given
20:19:23 From Shel Randall To Everyone:
	@Raj - true. I give a lot of feedback. Need a Dr who is tolerant of feedback
20:20:17 From Sophia Arellano To Everyone:
	do we need some neighbor information to complete that matrix? like if I win to a certain person and that person wins against a 3th person then there is high probability for me to beat tje 3th person?
20:20:20 From Raj Desikavinayagompillai To Everyone:
	Depends if the feedbacks were given to a recommender system or not if it is verbal feedbacks - not much luck
20:21:06 From Tripureswar Chattopadhaya To Everyone:
	How do we come to the estimate of the Probability ?
20:21:40 From Raj Desikavinayagompillai To Everyone:
	https://www.sciencedirect.com/science/article/pii/S2210832716000028
20:21:53 From Raj Desikavinayagompillai To Everyone:
	Real-time recommendation algorithms for crowdsourcing systems
20:21:55 From Neha Purohit To All Panelists:
	How is probability calculated for crowd sourcing?
20:22:02 From Daniel Politzer To All Panelists:
	https://devavrat.mit.edu/wp-content/uploads/2017/11/Budget-optimal-crowdsourcing-using-low-rank-matrix-approximations.pdf
20:22:27 From Neha Purohit To Everyone:
	How is probability calculated for crowd sourcing?
20:25:03 From Julie Mann To Everyone:
	Honestly, I'm just drawing a blank space on all this white space ....  not hitting my long term memory.
20:25:12 From Leandro Mbarak To Everyone:
	because if it is - the variance should increase with a bigger n, so we consider only + because the variance should decrease when n increase
20:25:23 From Valeria Herrera To All Panelists:
	Correction of standard error?
20:25:38 From Shel Randall To Everyone:
	like a root-mean-squared
20:26:39 From Louis Reid To Everyone:
	confidence level
20:29:58 From David Craig To Everyone:
	"The dreams of Reason (algorithms) are....nightmares!"  Francisco Goya
20:30:29 From Claudio Chaves To All Panelists:
	Q: Have you seen applications with tech support cases CSAT (customer satisfaction rate) applying completion matrix( just a small portion of cases receive a CSAT survey response) to find the correlations and recommendations around product category and case severity?
20:31:03 From Neha Purohit To Everyone:
	Q. How is Probability calculated for Crowd Sourcing ?
20:31:21 From Varuni Rao To Everyone:
	Q: Can we use matrix estimation to impute missing values in simple regression problems as well? If yes, how often? If no, why?
20:31:57 From Santiago Arroyo To Everyone:
	Q, Can you repeat the difference between prediction and decisión problem?
20:32:54 From Sophia Arellano To Everyone:
	Q: Do we need some neighbor information to complete that matrix? (the game matrix) like if I win person A and person A wins against person B, then I Will have a high probability for me to beat person B. How we come out with probabilities in game matrix without this kind of information?
20:37:11 From Angalar Chi To Everyone:
	for items, do they have to be objects or can they also be related to ideas or identities?
20:38:57 From Taylor Olson To Everyone:
	so these modules are the different ways to estimate the function described previously?
20:39:15 From Taylor Olson To Everyone:
	thank you
20:40:05 From Varuni Rao To Everyone:
	k means
20:40:05 From Raj Desikavinayagompillai To Everyone:
	K means
20:40:07 From Paula Valverde To Everyone:
	K-means
20:40:08 From Olamide Olowoyo To Everyone:
	knn
20:40:09 From Srihari Chelluri To All Panelists:
	k-means
20:40:10 From Yasir Maqbool To Everyone:
	knn
20:40:16 From Laura Gabrysiak To Everyone:
	kmeans and tsne
20:40:17 From Tripureswar Chattopadhaya To Everyone:
	K-Means
20:40:17 From Thierry Azalbert To Everyone:
	hierachical
20:40:25 From Lev Sukherman To Everyone:
	DBSCAN
20:40:32 From subhra roy To All Panelists:
	hierarchical clustering
20:40:42 From Lev Sukherman To Everyone:
	Hierarchical clustering
20:40:43 From Omar Alsaid Sulaiman To Everyone:
	knn is classification not clustering 🧐
20:41:03 From Lev Sukherman To Everyone:
	KNN can be used for regression as well
20:41:37 From Lev Sukherman To Everyone:
	Gaussian - Mixture Model
20:42:04 From Roman Neuhauser To Everyone:
	Q: Q: can you use clustering methods to predict Li,j?
20:43:34 From ashish kumar To Everyone:
	Q: are we clustering users and items independent of each other?
20:44:15 From ashish kumar To Everyone:
	i.e. we are not clustering based on the interaction between the users and items
20:45:05 From Varuni Rao To Everyone:
	Q: Sounds like segmentation of Users and items. How do we determine similarity?
20:45:07 From Taylor Olson To Everyone:
	you cluster based on what you do know, and then use the clusters to tell you what you dont know
20:45:09 From Daniel Gibson To Everyone:
	A: Find clusters by maximizing the AND function on the entries.
20:46:22 From Varuni Rao To Everyone:
	Q: what was the criterion for re-ordering?
20:46:57 From Varuni Rao To Everyone:
	thank you
20:47:52 From Jens Müller To Everyone:
	Looks not really constant ;-)
20:48:10 From Cristobal Fresno To Everyone:
	Q: how big needs to be the cluster?
20:48:11 From Jens Müller To Everyone:
	;-)
20:49:05 From Taylor Olson To Everyone:
	using a "local average" is a better "educated guess" than the average of all
20:49:22 From Dom Lazara To Everyone:
	Comment: So we use the real observations for a givne cluster and extrapolate to the rest of the missing values to complete the matrix.
20:49:56 From Raj Desikavinayagompillai To Everyone:
	Are we filling only with 1s
20:50:02 From Raj Desikavinayagompillai To Everyone:
	probabilities
20:50:30 From Leandro Mbarak To Everyone:
	Q. we have to replace even the previus data or just to fill the gaps?
20:50:53 From Filippo Caviglioni To Everyone:
	Q: can we use all types of averaging method?
20:51:13 From Fredy Vigo To Everyone:
	si this is basically a clustering problem, then we create the matches between groups
20:51:26 From Jaime Romanini To Everyone:
	Q: How do we fill the empty clusters?
20:51:31 From Johan Goedkoop To Everyone:
	press down average for unknows
20:51:37 From Juan Gallo To Everyone:
	So in area 1 all users are the same? and all movies are the same?
20:52:36 From Johan Goedkoop To Everyone:
	or that may be collab filt
20:52:39 From Jaime Romanini To Everyone:
	thanks
20:52:45 From Yasir Maqbool To Everyone:
	How these clusters ( squares) were formed?
20:54:45 From Jaime Romanini To Everyone:
	The users are not ordered
20:56:18 From Dom Lazara To Everyone:
	We can use features about the users that make them similar to do the mapping. Do we have features about the users in order to do this?
20:57:00 From Laura G To All Panelists:
	Q: Do we apply PCA for optimization then?
20:57:19 From Varuni Rao To Everyone:
	Q: Are we doing PCA on similarity matrix?
20:57:27 From Judith Marrugo To Everyone:
	Q. In Clustering Is this base on context ? I understod today we well not use context.
20:57:33 From Thierry Azalbert To Everyone:
	Q. : what happens if 2 users have no observed rating in common? How can we compute a similarity with no "relationship" between 2 users?
20:57:34 From Yasir Maqbool To Everyone:
	Q. Based on what we can find the similarity between users?
20:57:40 From Taylor Olson To Everyone:
	what about the rest of the users? The PCA of only 2 users might be flawed?
20:57:42 From Herman Gothe To Everyone:
	How do we build NxN matrix?
20:58:06 From Lev Sukherman To Everyone:
	Q: Can we use other dimensionality reduction methods?
20:58:16 From Herman Gothe To Everyone:
	suspense.....
20:58:46 From Lev Sukherman To Everyone:
	Q: Instead of PCA, can we use KPCA?
20:59:24 From Cristobal Fresno To Everyone:
	Shouldn’t we use Pricipal Coordinate Analysis instead of PCA?
20:59:44 From Laura G To All Panelists:
	Q: Could we run this method without dimensionality reduction and when using dimensionality reduction are there other methods?
21:02:28 From Neha Purohit To Everyone:
	Q. Why are we do doing PCA and K means both? PCA will created low dim space of users I.e. all users of similar attributes will be one PCA component.
21:03:37 From Mamta B To Everyone:
	Is there an issue audio?
21:04:36 From Moderator - Ankit Agrawal To Everyone:
	@Mamta: We are taking a 5min break
21:05:51 From Mamta B To Everyone:
	Ok Thanks !
21:06:31 From Cristobal Fresno To Everyone:
	Q: Shouldn’t we do  Principal Coordinate Analysis instead of Principal Component Analysis?
21:08:40 From Daniel Gibson To Everyone:
	Sum(And)
21:09:40 From Moderator - Ankit Agrawal To Everyone:
	Q. Why are we do doing PCA and K means both? PCA will created low dim space of users I.e. all users of similar attributes will be one PCA component.	A: PCA transforms data from high dimensions to low dimensions. All the users belong to the representation of all the components i.e. it creates new co-ordinate space for the data points. We still need to group them based on similarity after.
21:10:03 From Tripureswar Chattopadhaya To Everyone:
	Cosine distance
21:10:22 From Claudio Chaves To All Panelists:
	Q: Could we also use ratings (1-5 stars) Instead of 1 and -1?
21:11:43 From Julie Mann To Everyone:
	is this related to cosign correlation and the z score?
21:11:49 From Raj Desikavinayagompillai To Everyone:
	Does it mean my preference if I don’t select in Netflix will be recommended by neighbor preferences?
21:12:30 From Jens Müller To Everyone:
	Hence, 1 means both users are totally similar in terms of the given entries.
21:12:32 From Jens Müller To Everyone:
	?
21:12:42 From Laura G To All Panelists:
	Q: Arent missing values replaced by the local avg first?
21:15:10 From Sharon Kuang To Everyone:
	Q: How do we use this similarity matrix ?
21:15:18 From Santiago Arroyo To Everyone:
	Q. What if both users don't have common items without question marks?
21:15:29 From Kyle Ragaller To Everyone:
	would you please repeat that last part about the final purple matrix you created?
21:15:40 From Thierry Azalbert To Everyone:
	Q: so back to my earlier question: what happens if 2 users have no observed ratings in common?
21:16:03 From Jaime Romanini To Everyone:
	if one user does not have preferences how do we calculate the sim for that user among others?
21:16:14 From Helena Monteiro To Everyone:
	Q In very sparce database, will we have a similatity index for all pairs of users?
21:16:15 From Leandro Mbarak To Everyone:
	Q. I do not understand why u(1) = [-1,-1]
21:16:25 From Merrill Kashiwabara To Everyone:
	Can we give missing values a common value, to use “similarity of missing values” to calcualte similarity?
21:16:47 From Moderator - Ankit Agrawal To Everyone:
	Q: is this related to cosign correlation and the z score?	A: No. Basically you want to compute the angle between 2 users (vectors). Smaller the angle, closer the users are and larger the angle, farther away the users (vectors) are from each other
21:17:33 From Laura G To Everyone:
	Q: Meaning if the items are similar the same similarity can be inferred to the users?
21:18:12 From Moderator - Ankit Agrawal To Everyone:
	Q In very sparce database, will we have a similatity index for all pairs of users?	A: Good question. Sparse matrix is definitely a problem. We'll talk about SVD methods next to address that problem
21:18:29 From Daniel Gibson To Everyone:
	u(1) is not a function it is a subset vector
21:18:34 From Leandro Mbarak To Everyone:
	thanks
21:19:08 From Barbara Timm-Brock To Everyone:
	How is -1 + -1 + -1 + -1 = 2?
21:19:59 From Adithya Parvatam To Everyone:
	Its -1*-1 + -1*-1..(multiplication and addition)
21:20:06 From Daniel Landeros To Everyone:
	(-1)(-1) + (-1)(-1)
21:20:27 From Laura G To Everyone:
	Q: Are there other dimensionality reduction methods we can use (SVD,NMF, tSNE)? Is PCA the best method and then why?
21:20:38 From Barbara Timm-Brock To Everyone:
	THank you @Daniel!
21:20:39 From Jaime Romanini To Everyone:
	hamiltonian
21:21:24 From Sharon Kuang To Everyone:
	Q: What sigma stands for ?
21:21:38 From Cristobal Fresno To Everyone:
	The eigenvalue
21:21:54 From Varuni Rao To Everyone:
	Q: How did we decide upon 0.9 for r?
21:22:27 From Cristobal Fresno To Everyone:
	Arbitrary, so we loss little information
21:22:43 From Varuni Rao To Everyone:
	ok. thank you
21:22:52 From Jens Müller To Everyone:
	Yes why 0.9?
21:23:42 From simona tenaglia To Everyone:
	is sigma the standard deviation?
21:23:58 From Cristobal Fresno To Everyone:
	Sigma is the eigenvalue
21:24:08 From simona tenaglia To Everyone:
	thanks
21:25:11 From Varuni Rao To Everyone:
	Thank yoiu
21:25:19 From Moderator - Ankit Agrawal To Everyone:
	Q: Are there other dimensionality reduction methods we can use (SVD,NMF, tSNE)? Is PCA the best method and then why?	A: There are a lot of other methods available. We use PCA because it creates orthogonal dimensions while maximizing the variance which can be helpful in setting the threshold (in this case 0.9 i.e. 90% of variance captured)
21:27:06 From Herman Gothe To Everyone:
	Energy is equal to variance explained by components?
21:27:28 From Moderator - Ankit Agrawal To Everyone:
	Q: Energy is equal to variance explained by components?	A: Yes, in case of PCA
21:27:49 From Herman Gothe To Everyone:
	Thanks @Ankit!
21:30:10 From Shel Randall To Everyone:
	If you can master this, you can master quantum mechanics … they are so very similar it's scary.
21:30:40 From Judith Marrugo To Everyone:
	Sure Shel !!
21:31:28 From Mikael Friederich To Everyone:
	q. isn't parameter k likely different for users and items?
21:34:09 From Awad Alomari To Everyone:
	Q:: Could you please Professor reiterate how we come up with 2/3?
21:35:52 From Sharon Kuang To Everyone:
	Q: should k, j, be in same cluster ?
21:36:10 From Laura G To Everyone:
	Prof needs a bigger charger for Christmas ^^
21:36:19 From Judith Marrugo To Everyone:
	Q. Professor K is an hyperparameter?
21:36:51 From Yasir Maqbool To Everyone:
	Q. We have used users's rating for the products and based on the similarity we have proceeded. Can we use user's features to find similarity ( for e.g., age, gender, geography etc.) between users prior using with ratings.
21:37:59 From Taylor Olson To Everyone:
	is this the same method as the former, but forcing the number of users/items into the clusters?
21:38:03 From Jaime Romanini To Everyone:
	Why k is the same in ítems and users, it defines a radium?
21:40:38 From Filippo Caviglioni To Everyone:
	Q: n-Iterative SImilarity matrix can be obatined with S^n?
21:42:52 From owen sanford To Everyone:
	have to run to client meeting. Thank you, professor.
21:43:12 From Laura G To Everyone:
	Is there a python library most suitable for collaborative filtering that we can use?
21:43:17 From Laura G To Everyone:
	Q:
21:43:35 From Jaime Romanini To Everyone:
	why k is the same for users and ítems?
21:43:39 From Yan Li To All Panelists:
	Can you explain again interactive (the last one)?
21:44:01 From Moderator - Ankit Agrawal To Everyone:
	Q: Is there a python library most suitable for collaborative filtering that we can use?	A: We'll see implementation of it during MLS over the weekend.
21:44:13 From Judith Marrugo To Everyone:
	Jaime Romanini your question is excellent.
21:44:35 From Shel Randall To Everyone:
	@Laura - have a look at the New Package Introduction from the course resources.
21:44:55 From Laura G To Everyone:
	@Shel Let me check Thanks!
21:46:24 From Jaime Romanini To Everyone:
	Thanks @Judith
21:46:44 From Bas van Andel To Everyone:
	Q: Any recommended videos/literature on  clustering specifically in rec. systems/collaborative filtering?
21:47:09 From Yan Li To All Panelists:
	Where do we find these algorithms ?
21:48:28 From Thierry Azalbert To Everyone:
	@Bas: this Stanford series of 10 short videos covers this well. https://www.youtube.com/watch?v=1JRrCEgiyHM&list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV&index=41
21:50:03 From Varuni Rao To Everyone:
	Q: What is the purpose of dividing by p_hat?
21:50:30 From Paula Valverde To Everyone:
	Q: When do we apply SVD vs collaborate filtering?..just when a lot of entries are missing????…
21:50:42 From Bas van Andel To Everyone:
	Thanks Thierry
21:50:42 From Julio Rojas To Everyone:
	penalizes lack of info in the matrix?
21:50:49 From Cristobal Fresno To Everyone:
	Q: with SVD no similarity and k-nn needed, right?
21:51:12 From Daniel Gibson To Everyone:
	Just try them and pick the one that is less bad?
21:51:35 From Paula Valverde To Everyone:
	Thanks
21:51:36 From Jaime Romanini To Everyone:
	So k is because the matrix must be simmetric
21:51:43 From Lev Sukherman To Everyone:
	Q: Can we apply KL divergency to different clustering to check the distribution?
21:52:15 From Cristobal Fresno To Everyone:
	Q: how do we test the “accuracy” of the inputation?
21:53:55 From Varuni Rao To Everyone:
	Thank you. Got it
21:54:00 From Daniel Gibson To Everyone:
	@Cristobal I think that can only be done in the future when new data are collected based on the response to the recommendation.
21:54:08 From Lev Sukherman To Everyone:
	Q: Instead of SVD, can we apply QR decomposition and maybe it will work better?
21:54:55 From Srihari Chelluri To All Panelists:
	is there a minimum % of observations necessary to have good recommendation.
21:54:56 From Cristobal Fresno To Everyone:
	OK, on the same data we have already. Thanks
21:55:11 From Cristobal Fresno To Everyone:
	Thanks @Daniel
21:57:01 From Varuni Rao To Everyone:
	Q: So when we do SVD, we get U.S.V where S is a diagonal matrix. But, with SVT, we are not using the S diagonal matrix in calculations, right? Is sigma achieving the same as the S diagonal matrix?
21:58:01 From Judith Marrugo To Everyone:
	Q. Could this model be use to recomend a carrer to school students ?
21:58:06 From Varuni Rao To Everyone:
	ok . Thank you
21:58:09 From Johan Goedkoop To All Panelists:
	Did you talk about SVD advantages?
21:58:15 From Daniel Gibson To Everyone:
	career
21:59:00 From Judith Marrugo To Everyone:
	Thanks
21:59:09 From Laura G To Everyone:
	Thank you Prof!
21:59:10 From Adithya Parvatam To Everyone:
	Thanks Professor
21:59:16 From Daniel Politzer To Everyone:
	Thank you
21:59:17 From Lev Sukherman To Everyone:
	Thanks!
21:59:18 From ankita srivastava To Everyone:
	Thankyou professor
21:59:20 From Louis Reid To Everyone:
	Thank you Professor
21:59:35 From Sharon Kuang To Everyone:
	Thank you Professor
21:59:36 From Shel Randall To Everyone:
	ty
21:59:40 From Frances Rangel To Everyone:
	Thankyou Professor
21:59:41 From Michael Miller To Everyone:
	Thank You Professor 😁
21:59:41 From Dom Lazara To Everyone:
	Thank you Professor!
21:59:45 From Juan Gallo To Everyone:
	Great thank you professor!
21:59:46 From Johan Goedkoop To All Panelists:
	Thank you
21:59:55 From Varuni Rao To Everyone:
	Thank you Professor. Enjoyed the session!
21:59:55 From Jens Müller To Everyone:
	Thank you :-)
21:59:57 From Erwin Iost To Everyone:
	Thanks!!!
21:59:59 From Iwan Müller To Everyone:
	thank you Professor
22:00:08 From Fausto Correa To All Panelists:
	Thank you, Professor!!!
22:00:10 From Judith Marrugo To Everyone:
	Thanks Professor 😀
22:00:16 From Arturo Gudiño Chong To Everyone:
	Thank you!
22:00:17 From Helena Monteiro To Everyone:
	thank you!
22:00:18 From Jason Cuevas To Everyone:
	Thank you. Great Job
22:00:18 From Jose Rojas To Everyone:
	Thanks you Professor!
22:00:18 From Paula Valverde To Everyone:
	Thank you Professor! Great Lecture! Looking forward to tomorrow.
22:00:20 From Cristobal Fresno To Everyone:
	Thank you Professor
22:00:22 From Mahesh Ballani To Everyone:
	Its CWC semi final tomorrow :(
22:00:23 From Jean Vanlanduyt To Everyone:
	txs and good day
22:00:27 From Herman Gothe To Everyone:
	THANKS!
22:00:28 From Jakub Baranowski To Everyone:
	Thank you Professor!
22:00:33 From Chris Tecca To All Panelists:
	Thanks prof!
22:00:34 From Fernando Matias Gonzalez To Everyone:
	Thank you Professor
22:00:37 From Fredy Vigo To Everyone:
	thank you
22:00:43 From Han H To Everyone:
	Thank you professor!
22:00:47 From Jaime Romanini To Everyone:
	Thanks professor
22:00:47 From Vikas Srivastava To Everyone:
	Thank you Prof.
22:00:52 From Yadira M Del Rio To Everyone:
	Thank you Professor!
22:01:41 From Judith Marrugo To Everyone:
	Q. Can you repeat the question please?
22:02:29 From [gl mentor] nirupam sharma To Everyone:
	search about market basket analysis
22:02:41 From [gl mentor] nirupam sharma To Everyone:
	Apriori algorithm
22:03:01 From Laura G To Everyone:
	Q: Can we use SVD or Collaborative Filtering in other use cases e.g., location data represented in polygons?
22:09:11 From Yan Li To All Panelists:
	I mean the deduction of algorithm?
22:09:24 From Yan Li To All Panelists:
	Like how the algorithm gets computed
22:09:24 From [gl mentor] nirupam sharma To Everyone:
	https://www.youtube.com/watch?v=ee6E6aUGpm0
22:09:26 From Awad Alomari To Everyone:
	Thank you Ankit/Nirupam
22:09:39 From Judith Marrugo To Everyone:
	Q. Is it right to think  that recomendations system are more geographical dependent than others model ?
22:09:59 From Yan Li To All Panelists:
	Thank you!
22:12:09 From [gl mentor] nirupam sharma To Everyone:
	you can use it to represent extra courses
22:12:14 From [gl mentor] nirupam sharma To Everyone:
	open courses
22:13:49 From Yasir Maqbool To Everyone:
	How to evaluate recommendations?
22:14:48 From Judith Marrugo To Everyone:
	Q. I am confuse because the way u measure the error in this method
22:14:49 From Laura G To Everyone:
	Q: We had several questions if we need the dimensionality reduction step in the collaborative filter. If we need this step why and which dimensionality reduction algorithm is most suitable and why?
22:17:01 From Michael Miller To Everyone:
	IMHO, Unfortunately it seems the majority of recommendation systems are used for marketing to SELL MORE PRODUCTS and not provide the best recommendation for the user (consumer)
22:17:02 From Laura G To Everyone:
	Q follow up: So we apply dimensionality reduction for computational optimization. Thanks!
22:17:11 From [gl mentor] nirupam sharma To Everyone:
	yes
22:18:39 From Laura G To Everyone:
	Q: In the case of Amazon - I've seen several use cases when *after* purchasing a product I receive recommendations for variations of that same product type. Given that I have made my purchase already why do you think these recommendations are made?
22:19:02 From Azaria Berhane To Everyone:
	Thank you
22:20:31 From Michael Miller To Everyone:
	An Example is Amazon ALEXA Upselling, My Alexa reaches out to me to repurchase coffee or other products I previously purchases even when I don't want it, but its VERY HARD to disable the upselling feature and requires me to reconfigure from Alexa Dashboard.
22:23:06 From Oscar Guzman To Everyone:
	How and where (IT infraestructure resources required) this recommendations systems are allocated / integrated with CRM, ERP, modules?
22:23:14 From Laura G To Everyone:
	Thank you! Very interesting. Final questions - are there other use cases of recommendation systems in health care? Can we predict 'recommend' a sickness by the symptoms or similarity of users using this technique? Methodology feasibility/ Ethical issues? Thanks
22:25:40 From Daniel Politzer To Everyone:
	Gents, thank you for a great session. See you tomorrow
22:26:48 From Laura G To Everyone:
	Thanks
22:27:06 From Valeria Herrera To Everyone:
	thanks
22:27:09 From Michael Miller To Everyone:
	Is this our last week of classes
22:27:11 From Jens Müller To Everyone:
	Thank you very much :-)
22:27:12 From Varuni Rao To Everyone:
	Thank you Ankit and Niruppam
22:27:12 From Han H To Everyone:
	Thank you!
22:27:15 From Arturo Gudiño Chong To Everyone:
	Thanks!
22:27:15 From Eric Bonney To Everyone:
	thanks
22:27:20 From Cristobal Fresno To Everyone:
	Thank you mentors!!!!
22:27:20 From Herman Gothe To Everyone:
	Thanks!!
22:27:20 From Yadira M Del Rio To Everyone:
	Thank you
22:27:22 From Mayda Alkhaldi To Everyone:
	Thank you
22:27:23 From Merrill Kashiwabara To Everyone:
	Thanks!
22:27:26 From Judith Marrugo To Everyone:
	Thanks
22:27:27 From Jaime Romanini To Everyone:
	Thank you very much
22:27:29 From Gerald Ortiz To Everyone:
	Thank you
22:27:31 From JUAN RIOS To All Panelists:
	Thks !
22:27:31 From Fernando Matias Gonzalez To Everyone:
	Thank you!
22:27:32 From shikha sharma To Everyone:
	Thank you !
22:27:39 From Mario Lemos To All Panelists:
	thanks
22:27:42 From Michael Miller To Everyone:
	Thank You Both!
